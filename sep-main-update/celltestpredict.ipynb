{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.26.4\n",
      "openai==1.12.0\n",
      "tenacity==8.2.3\n",
      "tiktoken==0.6.0\n",
      "transformers==4.49.0\n",
      "pandas==2.2.0\n",
      "scikit-learn==1.4.0\n",
      "torch==2.5.0+cu124\n",
      "bitsandbytes==0.45.3\n",
      "datasets==3.3.2\n",
      "sentencepiece==0.2.0\n",
      "peft==0.14.0\n",
      "evaluate==0.4.3\n",
      "trl==0.15.2\n",
      "protobuf==4.25.2\n",
      "python-dotenv==1.0.1\n",
      "pandas_ta==0.3.14b0\n",
      "ollama==0.4.7\n",
      "accelerate==1.5.1\n",
      "ipywidgets==8.1.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_6244\\316775683.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "libs = [\n",
    "    \"numpy\", \"openai\", \"tenacity\", \"tiktoken\", \"transformers\", \"pandas\",\n",
    "    \"scikit-learn\", \"torch\", \"bitsandbytes\", \"datasets\", \"sentencepiece\",\n",
    "    \"peft\", \"evaluate\", \"trl\", \"protobuf\", \"python-dotenv\", \"pandas_ta\",\n",
    "    \"ollama\", \"accelerate\", \"ipywidgets\"\n",
    "]\n",
    "\n",
    "for lib in libs:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(lib).version\n",
    "        print(f\"{lib}=={version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{lib} not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4\n",
    "# !pip install openai==1.12.0\n",
    "# !pip install tenacity==8.2.3\n",
    "# !pip install tiktoken==0.6.0\n",
    "# # !pip install transformers==4.34.1\n",
    "# !pip install pandas==2.2.0\n",
    "# !pip install scikit-learn==1.4.0\n",
    "# # !pip install torch==2.2.0+cu118\n",
    "# # !pip install bitsandbytes==0.42.0\n",
    "# !pip install datasets==2.14.7\n",
    "# # !pip install sentencepiece==0.1.99\n",
    "# # !pip install peft==0.6.2\n",
    "# !pip install evaluate==0.4.1\n",
    "# !pip install trl==0.7.1\n",
    "# !pip install protobuf==4.25.2\n",
    "# !pip install python-dotenv\n",
    "# !pip install pandas_ta\n",
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install peft\n",
    "# !pip install evaluate\n",
    "# !pip install ipywidgets\n",
    "\n",
    "\n",
    "# # !pip install transformers==4.38.2\n",
    "# # !pip uninstall torch torchvision torchaudio -y\n",
    "# # !pip install torch==2.2.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# # !pip install -U bitsandbytes\n",
    "# # !pip install peft==0.6.2\n",
    "\n",
    "# !pip uninstall accelerate -y\n",
    "# !pip install accelerate\n",
    "\n",
    "# !pip install --upgrade accelerate\n",
    "# !pip install --upgrade torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torch==2.5.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torch-2.5.0%2Bcu124-cp312-cp312-win_amd64.whl (2510.7 MB)\n",
      "     ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.5 GB 14.3 MB/s eta 0:02:56\n",
      "     ---------------------------------------- 0.0/2.5 GB 41.9 MB/s eta 0:01:00\n",
      "     ---------------------------------------- 0.0/2.5 GB 45.2 MB/s eta 0:00:56\n",
      "      --------------------------------------- 0.0/2.5 GB 49.2 MB/s eta 0:00:51\n",
      "      --------------------------------------- 0.1/2.5 GB 55.3 MB/s eta 0:00:45\n",
      "     - -------------------------------------- 0.1/2.5 GB 63.4 MB/s eta 0:00:39\n",
      "     - -------------------------------------- 0.1/2.5 GB 72.6 MB/s eta 0:00:34\n",
      "     - -------------------------------------- 0.1/2.5 GB 76.4 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 0.1/2.5 GB 75.6 MB/s eta 0:00:32\n",
      "     -- ------------------------------------- 0.1/2.5 GB 73.4 MB/s eta 0:00:33\n",
      "     -- ------------------------------------- 0.2/2.5 GB 72.2 MB/s eta 0:00:33\n",
      "     -- ------------------------------------- 0.2/2.5 GB 72.9 MB/s eta 0:00:33\n",
      "     -- ------------------------------------- 0.2/2.5 GB 68.6 MB/s eta 0:00:34\n",
      "     --- ------------------------------------ 0.2/2.5 GB 72.4 MB/s eta 0:00:32\n",
      "     --- ------------------------------------ 0.2/2.5 GB 75.7 MB/s eta 0:00:31\n",
      "     --- ------------------------------------ 0.3/2.5 GB 78.4 MB/s eta 0:00:29\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 85.1 MB/s eta 0:00:27\n",
      "     ---- ----------------------------------- 0.3/2.5 GB 91.6 MB/s eta 0:00:25\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 94.1 MB/s eta 0:00:24\n",
      "     ----- ---------------------------------- 0.3/2.5 GB 94.7 MB/s eta 0:00:23\n",
      "     ----- ---------------------------------- 0.4/2.5 GB 94.7 MB/s eta 0:00:23\n",
      "     ------ --------------------------------- 0.4/2.5 GB 102.8 MB/s eta 0:00:21\n",
      "     ------ --------------------------------- 0.4/2.5 GB 110.3 MB/s eta 0:00:19\n",
      "     ------- -------------------------------- 0.4/2.5 GB 118.9 MB/s eta 0:00:18\n",
      "     ------- -------------------------------- 0.5/2.5 GB 118.9 MB/s eta 0:00:18\n",
      "     ------- -------------------------------- 0.5/2.5 GB 118.0 MB/s eta 0:00:18\n",
      "     -------- ------------------------------- 0.5/2.5 GB 118.1 MB/s eta 0:00:17\n",
      "     -------- ------------------------------- 0.5/2.5 GB 118.1 MB/s eta 0:00:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 118.0 MB/s eta 0:00:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 118.0 MB/s eta 0:00:17\n",
      "     --------- ------------------------------ 0.6/2.5 GB 118.9 MB/s eta 0:00:16\n",
      "     ---------- ----------------------------- 0.6/2.5 GB 118.9 MB/s eta 0:00:16\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 118.0 MB/s eta 0:00:16\n",
      "     ---------- ----------------------------- 0.7/2.5 GB 118.0 MB/s eta 0:00:16\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 118.0 MB/s eta 0:00:16\n",
      "     ----------- ---------------------------- 0.7/2.5 GB 118.0 MB/s eta 0:00:16\n",
      "     ------------ --------------------------- 0.8/2.5 GB 118.9 MB/s eta 0:00:15\n",
      "     ------------ --------------------------- 0.8/2.5 GB 118.9 MB/s eta 0:00:15\n",
      "     ------------ --------------------------- 0.8/2.5 GB 118.9 MB/s eta 0:00:15\n",
      "     ------------- -------------------------- 0.8/2.5 GB 118.0 MB/s eta 0:00:15\n",
      "     ------------- -------------------------- 0.9/2.5 GB 118.1 MB/s eta 0:00:14\n",
      "     -------------- ------------------------- 0.9/2.5 GB 118.1 MB/s eta 0:00:14\n",
      "     -------------- ------------------------- 0.9/2.5 GB 118.0 MB/s eta 0:00:14\n",
      "     -------------- ------------------------- 0.9/2.5 GB 118.0 MB/s eta 0:00:14\n",
      "     --------------- ------------------------ 1.0/2.5 GB 118.9 MB/s eta 0:00:14\n",
      "     --------------- ------------------------ 1.0/2.5 GB 118.9 MB/s eta 0:00:13\n",
      "     --------------- ------------------------ 1.0/2.5 GB 118.9 MB/s eta 0:00:13\n",
      "     ---------------- ----------------------- 1.0/2.5 GB 118.9 MB/s eta 0:00:13\n",
      "     ---------------- ----------------------- 1.1/2.5 GB 118.0 MB/s eta 0:00:13\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 118.0 MB/s eta 0:00:13\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 118.0 MB/s eta 0:00:12\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 118.1 MB/s eta 0:00:12\n",
      "     ----------------- ---------------------- 1.1/2.5 GB 106.8 MB/s eta 0:00:14\n",
      "     ------------------ --------------------- 1.1/2.5 GB 106.7 MB/s eta 0:00:13\n",
      "     ------------------ --------------------- 1.2/2.5 GB 106.1 MB/s eta 0:00:13\n",
      "     ------------------- -------------------- 1.2/2.5 GB 106.8 MB/s eta 0:00:13\n",
      "     ------------------- -------------------- 1.2/2.5 GB 106.7 MB/s eta 0:00:13\n",
      "     ------------------- -------------------- 1.2/2.5 GB 106.8 MB/s eta 0:00:12\n",
      "     -------------------- ------------------- 1.3/2.5 GB 106.8 MB/s eta 0:00:12\n",
      "     -------------------- ------------------- 1.3/2.5 GB 106.8 MB/s eta 0:00:12\n",
      "     -------------------- ------------------- 1.3/2.5 GB 106.8 MB/s eta 0:00:12\n",
      "     --------------------- ------------------ 1.3/2.5 GB 106.8 MB/s eta 0:00:11\n",
      "     --------------------- ------------------ 1.4/2.5 GB 106.8 MB/s eta 0:00:11\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 118.0 MB/s eta 0:00:10\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 118.0 MB/s eta 0:00:10\n",
      "     ---------------------- ----------------- 1.4/2.5 GB 118.9 MB/s eta 0:00:10\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 118.9 MB/s eta 0:00:09\n",
      "     ----------------------- ---------------- 1.5/2.5 GB 118.8 MB/s eta 0:00:09\n",
      "     ------------------------ --------------- 1.5/2.5 GB 118.1 MB/s eta 0:00:09\n",
      "     ------------------------ --------------- 1.5/2.5 GB 118.1 MB/s eta 0:00:09\n",
      "     ------------------------ --------------- 1.6/2.5 GB 118.0 MB/s eta 0:00:09\n",
      "     ------------------------- -------------- 1.6/2.5 GB 118.0 MB/s eta 0:00:08\n",
      "     ------------------------- -------------- 1.6/2.5 GB 118.0 MB/s eta 0:00:08\n",
      "     ------------------------- -------------- 1.6/2.5 GB 118.9 MB/s eta 0:00:08\n",
      "     -------------------------- ------------- 1.7/2.5 GB 118.9 MB/s eta 0:00:08\n",
      "     -------------------------- ------------- 1.7/2.5 GB 118.9 MB/s eta 0:00:07\n",
      "     --------------------------- ------------ 1.7/2.5 GB 118.0 MB/s eta 0:00:07\n",
      "     --------------------------- ------------ 1.7/2.5 GB 118.0 MB/s eta 0:00:07\n",
      "     --------------------------- ------------ 1.8/2.5 GB 118.0 MB/s eta 0:00:07\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 118.1 MB/s eta 0:00:07\n",
      "     ---------------------------- ----------- 1.8/2.5 GB 118.9 MB/s eta 0:00:06\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 118.8 MB/s eta 0:00:06\n",
      "     ----------------------------- ---------- 1.8/2.5 GB 118.1 MB/s eta 0:00:06\n",
      "     ----------------------------- ---------- 1.9/2.5 GB 118.9 MB/s eta 0:00:06\n",
      "     ------------------------------ --------- 1.9/2.5 GB 118.0 MB/s eta 0:00:06\n",
      "     ------------------------------ --------- 1.9/2.5 GB 118.0 MB/s eta 0:00:05\n",
      "     ------------------------------- -------- 1.9/2.5 GB 118.0 MB/s eta 0:00:05\n",
      "     ------------------------------- -------- 2.0/2.5 GB 118.9 MB/s eta 0:00:05\n",
      "     ------------------------------- -------- 2.0/2.5 GB 118.9 MB/s eta 0:00:05\n",
      "     -------------------------------- ------- 2.0/2.5 GB 118.9 MB/s eta 0:00:05\n",
      "     -------------------------------- ------- 2.0/2.5 GB 118.0 MB/s eta 0:00:04\n",
      "     -------------------------------- ------- 2.1/2.5 GB 118.0 MB/s eta 0:00:04\n",
      "     --------------------------------- ------ 2.1/2.5 GB 118.0 MB/s eta 0:00:04\n",
      "     --------------------------------- ------ 2.1/2.5 GB 118.1 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 2.1/2.5 GB 118.9 MB/s eta 0:00:04\n",
      "     ---------------------------------- ----- 2.2/2.5 GB 118.8 MB/s eta 0:00:03\n",
      "     ---------------------------------- ----- 2.2/2.5 GB 118.1 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 118.1 MB/s eta 0:00:03\n",
      "     ----------------------------------- ---- 2.2/2.5 GB 118.0 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 2.3/2.5 GB 118.0 MB/s eta 0:00:03\n",
      "     ------------------------------------ --- 2.3/2.5 GB 118.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 2.3/2.5 GB 118.0 MB/s eta 0:00:02\n",
      "     ------------------------------------ --- 2.3/2.5 GB 118.0 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 2.3/2.5 GB 105.4 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 2.4/2.5 GB 106.1 MB/s eta 0:00:02\n",
      "     ------------------------------------- -- 2.4/2.5 GB 105.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.4/2.5 GB 105.4 MB/s eta 0:00:02\n",
      "     -------------------------------------- - 2.4/2.5 GB 105.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.4/2.5 GB 105.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 105.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.5/2.5 GB 106.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 GB 17.7 MB/s eta 0:00:00\n",
      "Collecting torchvision==0.20.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchvision-0.20.0%2Bcu124-cp312-cp312-win_amd64.whl (6.1 MB)\n",
      "     ---------------------------------------- 0.0/6.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 6.1/6.1 MB 75.4 MB/s eta 0:00:00\n",
      "Collecting torchaudio==2.5.0\n",
      "  Downloading https://download.pytorch.org/whl/cu124/torchaudio-2.5.0%2Bcu124-cp312-cp312-win_amd64.whl (4.1 MB)\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 4.1/4.1 MB 82.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.0) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.0) (2023.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.0) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch==2.5.0) (1.13.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision==0.20.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision==0.20.0) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch==2.5.0) (3.0.2)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-2.5.0+cu124 torchaudio-2.5.0+cu124 torchvision-0.20.0+cu124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.2.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "GRID P40-24Q\n"
     ]
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "print(torch.cuda.is_available())  # Nếu trả về False, CUDA chưa hoạt động\n",
    "print(torch.cuda.device_count())  # Kiểm tra số lượng GPU\n",
    "print(torch.cuda.get_device_name(0))  # Hiển thị tên GPU\n",
    "# print(torch.set_default_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip show transformers bitsandbytes torch\n",
    "# !pip install accelerate\n",
    "# !pip install transformers --upgrade\n",
    "# !pip install peft --upgrade\n",
    "# !pip show peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2508f1ec462c44fdb470b8a0c18b80e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install --upgrade evaluate datasets\n",
    "\n",
    "# import evaluate\n",
    "# accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(price_dir='data/price/preprocessed/', tweet_dir='data/tweet/raw/', seq_len=5, wandb=False, data_path='./data/merge_sample.json', output_path='./saved_models/lora-Vicuna', model_path='lmsys/vicuna-7b-v1.5-16k', eval_steps=200, save_steps=200, resume_from_supervised_checkpoint=None, ignore_data_skip='False', num_reflect_trials=2, datasets_dir='./datasets/', local_rank=0, resume_from_reward_checkpoint=False, deepspeed=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, reward_gradient_accumulation_steps=32, reward_learning_rate=2e-05, weight_decay=0.001, reward_base_model='lmsys/vicuna-7b-v1.5-16k', bf16=False, num_train_epochs=1, train_subset=100000, eval_subset=50000, gradient_checkpointing=False, optim='adamw_hf', lr_scheduler_type='linear', reward_adapter='./saved_models/reward_model_vicuna-7b', rl_base_model='./saved_models/lora-Vicuna-adapter-merged', tokenizer_name='lmsys/vicuna-7b-v1.5-16k', reward_model_name='./saved_models/reward_model_vicuna-7b-adapter-merged', log_with=None, rl_learning_rate=1.4e-05, output_max_length=128, mini_batch_size=1, batch_size=1, ppo_epochs=4, rl_gradient_accumulation_steps=1, adafactor=False, early_stopping=True, target_kl=0.1, reward_baseline=0, batched_gen=True, save_freq=None, output_dir='./saved_models/tuning_llama_rl_checkpoints/', seed=0, num_shots=4, save_dir='results/')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/merge_sample.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "fix_seed = 100\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    price_dir=\"data/price/preprocessed/\",\n",
    "    tweet_dir=\"data/tweet/raw/\",\n",
    "    seq_len=5,\n",
    "    wandb=False,\n",
    "    data_path=\"./data/merge_sample.json\",\n",
    "    output_path=\"./saved_models/lora-Vicuna\",\n",
    "    model_path=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    resume_from_supervised_checkpoint=None,\n",
    "    ignore_data_skip=\"False\",\n",
    "    num_reflect_trials=2,\n",
    "    datasets_dir=\"./datasets/\",\n",
    "    local_rank=0,\n",
    "    resume_from_reward_checkpoint=False,\n",
    "    deepspeed=None,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    reward_gradient_accumulation_steps=32,\n",
    "    reward_learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    reward_base_model=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    bf16=False,\n",
    "    num_train_epochs=1,\n",
    "    train_subset=100000,\n",
    "    eval_subset=50000,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_hf\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    reward_adapter=\"./saved_models/reward_model_vicuna-7b\",\n",
    "    rl_base_model=\"./saved_models/lora-Vicuna-adapter-merged\",\n",
    "    tokenizer_name=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    reward_model_name=\"./saved_models/reward_model_vicuna-7b-adapter-merged\",\n",
    "    log_with=None,\n",
    "    rl_learning_rate=1.4e-5,\n",
    "    output_max_length=128,\n",
    "    mini_batch_size=1,\n",
    "    batch_size=1,\n",
    "    ppo_epochs=4,\n",
    "    rl_gradient_accumulation_steps=1,\n",
    "    adafactor=False,\n",
    "    early_stopping=True,\n",
    "    target_kl=0.1,\n",
    "    reward_baseline=0,\n",
    "    batched_gen=True,\n",
    "    save_freq=None,\n",
    "    output_dir=\"./saved_models/tuning_llama_rl_checkpoints/\",\n",
    "    seed=0,\n",
    "    num_shots=4,\n",
    "    save_dir=\"results/\"\n",
    ")\n",
    "\n",
    "print(\"Args in experiment:\")\n",
    "print(args)\n",
    "\n",
    "args.data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: lmsys/vicuna-7b-v1.5-16k\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174b2e467a5a4465b607e3c069235536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,194,304 || all params: 6,742,609,920 || trainable%: 0.0622\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " If there's a warning about missing keys above, please disregard :)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:26, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_config:  LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='lmsys/vicuna-7b-v1.5-16k', revision=None, inference_mode=True, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398d1e714bba4ed3b5aec37fbc429afa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight']\n",
      "  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'do_sample': True}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    prepare_model_for_kbit_training, ##***\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import bitsandbytes as bnb\n",
    "import transformers\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset\n",
    "from predict_module import sft_dataloader\n",
    "\n",
    "\n",
    "# def check_quantization(model):\n",
    "#     if hasattr(model, \"quantization_method\"):\n",
    "#         print(f\"Mô hình đã được lượng tử hóa với phương pháp: {model.quantization_method}\")\n",
    "#         return True\n",
    "#     else:\n",
    "#         print(\"Mô hình không được lượng tử hóa.\")\n",
    "#         return False\n",
    "\n",
    "\n",
    "# def get_model_size(model):\n",
    "#     # Đếm tổng số tham số\n",
    "#     total_params = sum(p.numel() for p in model.parameters())\n",
    "#     print(f\"Tổng số tham số: {total_params:,}\")\n",
    "\n",
    "#     # Ước lượng kích thước lý thuyết (trong GB)\n",
    "#     # Với lượng tử hóa 4-bit, mỗi tham số chiếm 0.5 byte (4 bit = 0.5 byte)\n",
    "#     if check_quantization(model):\n",
    "#         size_in_bytes = total_params * 0.5  # 4-bit\n",
    "#         print(\"Kích thước lý thuyết (4-bit): {:.2f} GB\".format(size_in_bytes / 1024**3))\n",
    "#     else:\n",
    "#         # Nếu không lượng tử hóa, giả sử float16 (4 byte mỗi tham số)\n",
    "#         size_in_bytes = total_params * 2  # float16\n",
    "#         print(\"Kích thước lý thuyết (float16): {:.2f} GB\".format(size_in_bytes / 1024**3))\n",
    "\n",
    "#     # Kiểm tra kích thước thực tế trên GPU (nếu có)\n",
    "#     if torch.cuda.is_available():\n",
    "#         torch.cuda.empty_cache()\n",
    "#         model = model.to(\"cuda\")\n",
    "#         memory_used = torch.cuda.memory_allocated() / 1024**3  # Đổi sang GB\n",
    "#         print(f\"Kích thước thực tế trên GPU: {memory_used:.2f} GB\")\n",
    "#     else:\n",
    "#         print(\"Không có GPU để kiểm tra kích thước thực tế.\")\n",
    "\n",
    "\n",
    "\n",
    "def supervised_finetune(args):\n",
    "    # Các hằng số huấn luyện\n",
    "    MICRO_BATCH_SIZE = 4\n",
    "    BATCH_SIZE = 128\n",
    "    MAX_STEPS = None\n",
    "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "    EPOCHS = 2\n",
    "    LEARNING_RATE = 3e-4\n",
    "    CUTOFF_LEN = 256\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    VAL_PCT = 0.1\n",
    "    TARGET_MODULES = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ]\n",
    "    DATA_PATH = args.data_path\n",
    "    OUTPUT_DIR = args.output_path\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "    # Xử lý phân tán (Distributed Data Parallel - DDP)\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        torch.cuda.set_device(int(os.environ.get(\"LOCAL_RANK\", 0)))\n",
    "        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    "\n",
    "    # In đường dẫn mô hình để kiểm tra\n",
    "    print(f\"Loading model from: {args.model_path}\")\n",
    "\n",
    "    # Kiểm tra GPU có sẵn\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This script requires a GPU with bitsandbytes support.\")\n",
    "\n",
    "    # # Cấu hình lượng tử hóa 4-bit với BitsAndBytesConfig\n",
    "    # quant_config = BitsAndBytesConfig(\n",
    "    #     load_in_4bit=True,\n",
    "    #     bnb_4bit_quant_type=\"nf4\",\n",
    "    #     bnb_4bit_compute_dtype=torch.float16\n",
    "    # )\n",
    "\n",
    "    # Tải tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        args.model_path,\n",
    "        add_eos_token=True,\n",
    "        local_files_only=args.offline if hasattr(args, 'offline') else False\n",
    "    )\n",
    "\n",
    "    ##====================================================================\n",
    "\n",
    "    # Tải mô hình mà không dùng device_map hoặc bất kỳ cơ chế phân phối nào từ accelerate\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        # quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=None,\n",
    "        low_cpu_mem_usage=True,  # Tắt cơ chế tự động phân phối của accelerate\n",
    "        # Không thêm device_map hoặc bất kỳ tham số nào có thể kích hoạt accelerate\n",
    "    )\n",
    "\n",
    "\n",
    "    # # Kiểm tra lượng tử hóa và kích thước mô hình trước khi áp dụng LoRA\n",
    "    # print(\"\\nKích thước mô hình gốc:\")\n",
    "    # get_model_size(model)\n",
    "\n",
    "    # check_quantization(model)\n",
    "    # model.gradient_checkpointing_enable()\n",
    "    # model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "\n",
    "\n",
    "    # Cấu hình LoRA cho fine-tuning\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    # # Kiểm tra kích thước sau LoRA\n",
    "    # print(\"\\nKích thước mô hình sau khi áp dụng LoRA:\")\n",
    "    # get_model_size(model)\n",
    "    model.print_trainable_parameters()\n",
    "    ##====================================================================\n",
    "\n",
    "\n",
    "    \n",
    "    data = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "    val_set_size = VAL_PCT * len(data)\n",
    "    print(data)\n",
    "    now_max_steps = max(\n",
    "        (len(data[\"train\"]) - val_set_size) // BATCH_SIZE * EPOCHS, EPOCHS)\n",
    "\n",
    "    if args.resume_from_supervised_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            args.resume_from_supervised_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            pytorch_bin_path = checkpoint_name\n",
    "            checkpoint_name = os.path.join(\n",
    "                args.resume_from_supervised_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            if os.path.exists(checkpoint_name):\n",
    "                os.rename(checkpoint_name, pytorch_bin_path)\n",
    "                warnings.warn(\n",
    "                    \"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n",
    "            else:\n",
    "                args.resume_from_supervised_checkpoint = (\n",
    "                    None  # So the trainer won't try loading its state\n",
    "                )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "        train_args_path = os.path.join(\n",
    "            resume_from_checkpoint, \"trainer_state.json\")\n",
    "\n",
    "        if os.path.exists(train_args_path):\n",
    "            import json\n",
    "            base_train_args = json.load(open(train_args_path, 'r'))\n",
    "            base_max_steps = base_train_args[\"max_steps\"]\n",
    "            resume_scale = base_max_steps / now_max_steps\n",
    "            if base_max_steps > now_max_steps:\n",
    "                warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(\n",
    "                    EPOCHS, base_max_steps))\n",
    "                EPOCHS = None\n",
    "                MAX_STEPS = base_max_steps\n",
    "            else:\n",
    "                MAX_STEPS = now_max_steps\n",
    "    else:\n",
    "        MAX_STEPS = now_max_steps\n",
    "\n",
    "    # model.print_trainable_parameters()\n",
    "    dataloader = sft_dataloader.SFTDataLoader(\n",
    "        data, CUTOFF_LEN, val_set_size, tokenizer)\n",
    "    train_data, val_data = dataloader.load_data()\n",
    "    # print(train_data)\n",
    "    # print(val_data)\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            max_steps=MAX_STEPS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            fp16=True,\n",
    "            logging_steps=20,\n",
    "            eval_strategy =\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=args.eval_steps if val_set_size > 0 else None,\n",
    "            save_steps=args.save_steps,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            save_total_limit=30,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            report_to=\"wandb\" if args.wandb else [],\n",
    "            ignore_data_skip=args.ignore_data_skip,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    "    ).__get__(model, type(model))\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    print(\"\\n If there's a warning about missing keys above, please disregard :)\")\n",
    "\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        trainer.train(resume_from_checkpoint=args.resume_from_supervised_checkpoint)\n",
    "\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "\n",
    "from predict_module.merge_peft_adapter import merge_peft_adapter\n",
    "\n",
    "# Train supervised policy\n",
    "supervised_finetune(args)\n",
    "merge_peft_adapter(model_name=args.output_path, output_name=args.rl_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name:  ./datasets/\n",
      "device_map:  auto\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa82f7934e6b437fa989ae43eb2e7f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at lmsys/vicuna-7b-v1.5-16k and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,198,400 || all params: 6,611,546,112 || trainable%: 0.0635\n",
      "train_dataset:  6\n",
      "train_dataset:  6\n",
      "eval_dataset:  6\n",
      "eval_dataset:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2708: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last checkpoint of the model\n",
      "peft_config:  LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='lmsys/vicuna-7b-v1.5-16k', revision=None, inference_mode=True, r=8, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, eva_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a60b48c1084105b913869b839d12aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'do_sample': True}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, LlamaForSequenceClassification, LlamaConfig\n",
    "\n",
    "from predict_module import rm_dataloader\n",
    "\n",
    "# DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "# DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "\n",
    "\n",
    "def train_reward_model(args):\n",
    "    script_args = args\n",
    "\n",
    "    dataset_name = script_args.datasets_dir\n",
    "    print(\"dataset_name: \", dataset_name)\n",
    "\n",
    "    # Define the training args. Needs to be done before the model is loaded if you are using deepspeed.\n",
    "    # model_name_split = script_args.reward_base_model.split(\"/\")[-1]\n",
    "    # output_name = (\n",
    "    # f\"{model_name_split}_peft_stack-exchange-paired_rmts__{script_args.train_subset}_{script_args.reward_learning_rate}\"\n",
    "    # )\n",
    "    output_name = script_args.reward_adapter\n",
    "\n",
    "    # Define the training args. Needs to be done before the model is loaded if you are using deepspeed.\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_name,\n",
    "        learning_rate=script_args.reward_learning_rate,\n",
    "        per_device_train_batch_size=script_args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=script_args.per_device_eval_batch_size,\n",
    "        num_train_epochs=script_args.num_train_epochs,\n",
    "        weight_decay=script_args.weight_decay,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,  # 500,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,  # 500,\n",
    "        save_total_limit=2,\n",
    "        gradient_accumulation_steps=script_args.reward_gradient_accumulation_steps,\n",
    "        gradient_checkpointing= True, #script_args.gradient_checkpointing,\n",
    "        deepspeed=script_args.deepspeed,\n",
    "        # local_rank=script_args.local_rank,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[],\n",
    "        # bf16=script_args.bf16,\n",
    "        # fp16=True,\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        optim=script_args.optim,\n",
    "        lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    # Load the value-head model and tokenizer.\n",
    "    if \"llama\" in script_args.reward_base_model or \"vicuna\" in script_args.reward_base_model or \"Vicuna\" in script_args.reward_base_model:\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(script_args.reward_base_model)\n",
    "        config = LlamaConfig.from_pretrained(script_args.reward_base_model)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(script_args.reward_base_model, trust_remote_code=True)\n",
    "        config = AutoConfig.from_pretrained(script_args.reward_base_model, trust_remote_code=True)\n",
    "\n",
    "    # if \"llama\" in script_args.reward_base_model or \"vicuna\" in script_args.reward_base_model or \"Vicuna\" in script_args.reward_base_model:\n",
    "    #     # required for llama\n",
    "    #     tokenizer.add_special_tokens(\n",
    "    #         {\n",
    "    #             \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "    #             \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "    #             \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    #             \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "    #         }\n",
    "    #     )\n",
    "    # else:\n",
    "    #     # required for gpt2\n",
    "    #     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    print(\"device_map: \", device_map)\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    #    script_args.reward_base_model, num_labels=1, torch_dtype=torch.bfloat16\n",
    "    # )\n",
    "\n",
    "    if \"llama\" in script_args.reward_base_model or \"vicuna\" in script_args.reward_base_model or \"Vicuna\" in script_args.reward_base_model:\n",
    "        model = LlamaForSequenceClassification.from_pretrained(\n",
    "            script_args.reward_base_model,\n",
    "            num_labels=1,\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    else:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            script_args.reward_base_model,\n",
    "            num_labels=1,\n",
    "            torch_dtype=torch.float16,\n",
    "            trust_remote_code=True,\n",
    "        )\n",
    "\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=16,  # 32,\n",
    "        lora_dropout=0.05,  # 0.1,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Need to do this for gpt2, because it doesn't have an official pad token.\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    model.config.use_cache = not script_args.gradient_checkpointing\n",
    "    num_proc = 1  # Can adjust to be higher if you have more processors.\n",
    "\n",
    "\n",
    "    reward_dataloder = rm_dataloader.RewardDataLoader(dataset_name, script_args.train_subset, script_args.eval_subset, num_proc, tokenizer)\n",
    "    train_dataset, eval_dataset = reward_dataloder.load_data()\n",
    "\n",
    "    @dataclass\n",
    "    class RewardDataCollatorWithPadding:\n",
    "        tokenizer: PreTrainedTokenizerBase\n",
    "        padding: Union[bool, str, PaddingStrategy] = True\n",
    "        max_length: Optional[int] = None\n",
    "        pad_to_multiple_of: Optional[int] = None\n",
    "        return_tensors: str = \"pt\"\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "            features_j = []\n",
    "            features_k = []\n",
    "            for feature in features:\n",
    "                features_j.append(\n",
    "                    {\n",
    "                        \"input_ids\": feature[\"input_ids_j\"],\n",
    "                        \"attention_mask\": feature[\"attention_mask_j\"],\n",
    "                    }\n",
    "                )\n",
    "                features_k.append(\n",
    "                    {\n",
    "                        \"input_ids\": feature[\"input_ids_k\"],\n",
    "                        \"attention_mask\": feature[\"attention_mask_k\"],\n",
    "                    }\n",
    "                )\n",
    "            batch_j = self.tokenizer.pad(\n",
    "                features_j,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=self.return_tensors,\n",
    "            )\n",
    "            batch_k = self.tokenizer.pad(\n",
    "                features_k,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=self.return_tensors,\n",
    "            )\n",
    "            batch = {\n",
    "                \"input_ids_j\": batch_j[\"input_ids\"],\n",
    "                \"attention_mask_j\": batch_j[\"attention_mask\"],\n",
    "                \"input_ids_k\": batch_k[\"input_ids\"],\n",
    "                \"attention_mask_k\": batch_k[\"attention_mask\"],\n",
    "                \"return_loss\": True,\n",
    "            }\n",
    "            return batch\n",
    "\n",
    "\n",
    "    # Define the metric that we'll use for validation.\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, _ = eval_pred\n",
    "        # Here, predictions is rewards_j and rewards_k.\n",
    "        # We want to see how much of the time rewards_j > rewards_k.\n",
    "        predictions = np.argmax(predictions, axis=0)\n",
    "        labels = np.zeros(predictions.shape)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "    class RewardTrainer(Trainer):\n",
    "        # Define how to compute the reward loss. We use the InstructGPT pairwise logloss: https://arxiv.org/abs/2203.02155\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            rewards_j = model(\n",
    "                input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n",
    "            rewards_k = model(\n",
    "                input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n",
    "            loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "            if return_outputs:\n",
    "                return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "            return loss\n",
    "\n",
    "\n",
    "    # Train the model, woohoo.\n",
    "    trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=RewardDataCollatorWithPadding(\n",
    "            tokenizer=tokenizer, max_length=512, pad_to_multiple_of=8),\n",
    "    )\n",
    "\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    trainer.train(script_args.resume_from_reward_checkpoint)\n",
    "\n",
    "    print(\"Saving last checkpoint of the model\")\n",
    "    # model.save_pretrained(output_name + \"_peft_last_checkpoint\")\n",
    "    model.save_pretrained(output_name)\n",
    "\n",
    "\n",
    "train_reward_model(args)\n",
    "\n",
    "from predict_module.merge_peft_adapter import merge_peft_adapter\n",
    "merge_peft_adapter(model_name=args.reward_adapter, output_name=args.reward_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./saved_models/reward_model_vicuna-7b-adapter-merged\n",
      "dataset_name:  ./datasets/\n",
      "./saved_models/lora-Vicuna-adapter-merged\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1434a33655c74a428d673ba878785275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from './saved_models/lora-Vicuna-adapter-merged', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finetune model:  ./saved_models/lora-Vicuna-adapter-merged <class 'trl.models.modeling_value_head.AutoModelForCausalLMWithValueHead'>\n",
      "reward_model_name:  ./saved_models/reward_model_vicuna-7b-adapter-merged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7958221b563540e590b1a11a955c7f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at ./saved_models/reward_model_vicuna-7b-adapter-merged and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_model:  <class 'transformers.models.llama.modeling_llama.LlamaForSequenceClassification'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769041f54e584aa18520bb193e9aa51e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model class: <class 'trl.models.modeling_value_head.AutoModelForCausalLMWithValueHead'>\n",
      "Reward model class: <class 'transformers.models.llama.modeling_llama.LlamaForSequenceClassification'>\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 20.07 GiB is allocated by PyTorch, and 1.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 274\u001b[39m\n\u001b[32m    270\u001b[39m         ppo_trainer.save_pretrained(script_args.output_dir + \u001b[33m\"\u001b[39m\u001b[33mstep_saved\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# Optimize using reinforcement learning\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m \u001b[43mtuning_lm_with_rl\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpredict_module\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge_peft_adapter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge_peft_adapter\n\u001b[32m    277\u001b[39m merge_peft_adapter(model_name=args.output_dir+\u001b[33m\"\u001b[39m\u001b[33mstep_saved\u001b[39m\u001b[33m\"\u001b[39m, output_name=\u001b[33m\"\u001b[39m\u001b[33m./saved_models/sep_model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 201\u001b[39m, in \u001b[36mtuning_lm_with_rl\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    193\u001b[39m     optimizer = Adafactor(\n\u001b[32m    194\u001b[39m         \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m p: p.requires_grad, model.parameters()),\n\u001b[32m    195\u001b[39m         scale_parameter=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    198\u001b[39m         lr=config.learning_rate,\n\u001b[32m    199\u001b[39m     )\n\u001b[32m    200\u001b[39m \u001b[38;5;66;03m# We then build the PPOTrainer, passing the model, the reference model, the tokenizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m ppo_trainer = \u001b[43mPPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_model\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mref_model\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcollator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# We then build the sentiment analysis pipeline, passing the model name and the\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;66;03m# sentiment analysis pipeline arguments. Let's also make sure to set the device\u001b[39;00m\n\u001b[32m    215\u001b[39m \u001b[38;5;66;03m# to the same device as the PPOTrainer.\u001b[39;00m\n\u001b[32m    216\u001b[39m device = ppo_trainer.accelerator.device\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\ppo_trainer.py:273\u001b[39m, in \u001b[36mPPOTrainer.__init__\u001b[39m\u001b[34m(self, args, processing_class, model, ref_model, reward_model, train_dataset, value_model, data_collator, eval_dataset, optimizers, callbacks, peft_config)\u001b[39m\n\u001b[32m    270\u001b[39m \u001b[38;5;66;03m# sync random states for DataLoader(shuffle=True) before `accelerator.prepare`\u001b[39;00m\n\u001b[32m    271\u001b[39m \u001b[38;5;66;03m# see https://gist.github.com/vwxyzjn/2581bff1e48e185e0b85b6dfe1def79c\u001b[39;00m\n\u001b[32m    272\u001b[39m torch.manual_seed(args.seed)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.optimizer, \u001b[38;5;28mself\u001b[39m.dataloader = \u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m torch.manual_seed(\u001b[38;5;28mself\u001b[39m.local_seed)  \u001b[38;5;66;03m# reset the local seed again\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;28mself\u001b[39m.eval_dataloader = DataLoader(\n\u001b[32m    277\u001b[39m     \u001b[38;5;28mself\u001b[39m.eval_dataset,\n\u001b[32m    278\u001b[39m     batch_size=args.per_device_eval_batch_size,\n\u001b[32m    279\u001b[39m     collate_fn=\u001b[38;5;28mself\u001b[39m.data_collator,\n\u001b[32m    280\u001b[39m     drop_last=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    281\u001b[39m )  \u001b[38;5;66;03m# no need to shuffle eval dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:1398\u001b[39m, in \u001b[36mAccelerator.prepare\u001b[39m\u001b[34m(self, device_placement, *args)\u001b[39m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp8_backend == \u001b[33m\"\u001b[39m\u001b[33mMSAMP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m         args, device_placement = \u001b[38;5;28mself\u001b[39m._prepare_msamp(*args, device_placement=device_placement)\n\u001b[32m-> \u001b[39m\u001b[32m1398\u001b[39m     result = \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1401\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m._prepare_one(obj, device_placement=d) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[32m   1402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[32m   1403\u001b[39m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:1399\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp8_backend == \u001b[33m\"\u001b[39m\u001b[33mMSAMP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1397\u001b[39m         args, device_placement = \u001b[38;5;28mself\u001b[39m._prepare_msamp(*args, device_placement=device_placement)\n\u001b[32m   1398\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\n\u001b[32m-> \u001b[39m\u001b[32m1399\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_pass\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(args, device_placement)\n\u001b[32m   1400\u001b[39m     )\n\u001b[32m   1401\u001b[39m     result = \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m._prepare_one(obj, device_placement=d) \u001b[38;5;28;01mfor\u001b[39;00m obj, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, device_placement))\n\u001b[32m   1402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tpu_should_fix_optimizer:\n\u001b[32m   1403\u001b[39m     \u001b[38;5;66;03m# 2. grabbing new model parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:1272\u001b[39m, in \u001b[36mAccelerator._prepare_one\u001b[39m\u001b[34m(self, obj, first_pass, device_placement)\u001b[39m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prepare_data_loader(obj, device_placement=device_placement)\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch.nn.Module):\n\u001b[32m-> \u001b[39m\u001b[32m1272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprepare_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_placement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1273\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, torch.optim.Optimizer):\n\u001b[32m   1274\u001b[39m     optimizer = \u001b[38;5;28mself\u001b[39m.prepare_optimizer(obj, device_placement=device_placement)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:1518\u001b[39m, in \u001b[36mAccelerator.prepare_model\u001b[39m\u001b[34m(self, model, device_placement, evaluation_mode)\u001b[39m\n\u001b[32m   1513\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1514\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt train a model that has been loaded in 8-bit or 4-bit precision with CPU or disk offload. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1515\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf you want train the 8-bit or 4-bit model in CPU, please install bitsandbytes with multi-backend, see https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1516\u001b[39m         )\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_placement \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verify_device_map(model):\n\u001b[32m-> \u001b[39m\u001b[32m1518\u001b[39m     model = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1519\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_mode:\n\u001b[32m   1520\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.distributed_type \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m   1521\u001b[39m         DistributedType.MULTI_GPU,\n\u001b[32m   1522\u001b[39m         DistributedType.MULTI_MLU,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1527\u001b[39m         DistributedType.MULTI_HPU,\n\u001b[32m   1528\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1340\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1338\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1340\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 900 (3 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:900\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    904\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    905\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    910\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    911\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:927\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    930\u001b[39m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1326\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1320\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1321\u001b[39m             device,\n\u001b[32m   1322\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1323\u001b[39m             non_blocking,\n\u001b[32m   1324\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1325\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 20.07 GiB is allocated by PyTorch, and 1.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, HfArgumentParser, pipeline, BitsAndBytesConfig\n",
    "from transformers import LlamaTokenizer, LlamaConfig, LlamaForSequenceClassification, LlamaForCausalLM\n",
    "\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from trl.core import LengthSampler\n",
    "import os\n",
    "\n",
    "from transformers import GenerationConfig, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "# DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "# DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def tuning_lm_with_rl(args):\n",
    "    script_args = args\n",
    "\n",
    "    reward_model_name = script_args.reward_model_name\n",
    "    print(reward_model_name)\n",
    "\n",
    "\n",
    "    # dataset_name = \"lvwerra/stack-exchange-paired\"\n",
    "    dataset_name = script_args.datasets_dir\n",
    "    print(\"dataset_name: \", dataset_name)\n",
    "\n",
    "    config = PPOConfig(\n",
    "        learning_rate=script_args.rl_learning_rate,\n",
    "        batch_size=script_args.batch_size,\n",
    "        mini_batch_size=script_args.mini_batch_size,\n",
    "        gradient_accumulation_steps=script_args.rl_gradient_accumulation_steps,\n",
    "        num_ppo_epochs=script_args.ppo_epochs,\n",
    "        seed=script_args.seed,\n",
    "    )\n",
    "    model_name=script_args.rl_base_model\n",
    "    print(model_name)\n",
    "\n",
    "    # train_dataset = load_dataset(\"lvwerra/stack-exchange-paired\", data_dir=\"data/rl\", split=\"train\")\n",
    "    # train_dataset = train_dataset.select(range(100000))\n",
    "    train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    # train_dataset = train_dataset.select(range(100))\n",
    "    # We then define the arguments to pass to the sentiment analysis pipeline.\n",
    "    # We set `return_all_scores` to True to get the sentiment score for each token.\n",
    "    # sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16, \"truncation\": True}\n",
    "    sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 1, \"truncation\": True}\n",
    "\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name)\n",
    "    # GPT-2 tokenizer has a pad token, but it is not eos_token by default. We need to set it to eos_token.\n",
    "    # only for this model.\n",
    "\n",
    "    if \"llama\" in script_args.tokenizer_name or \"vicuna\" in script_args.rl_base_model or \"Vicuna\" in script_args.rl_base_model:\n",
    "        tokenizer = LlamaTokenizer.from_pretrained(script_args.tokenizer_name)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name)\n",
    "\n",
    "\n",
    "    # if \"llama\" in script_args.tokenizer_name or \"vicuna\" in script_args.rl_base_model or \"Vicuna\" in script_args.rl_base_model:\n",
    "    #     tokenizer.add_special_tokens(\n",
    "    #         {\n",
    "    #             \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "    #             \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "    #             \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "    #             \"pad_token\": DEFAULT_PAD_TOKEN,\n",
    "    #         }\n",
    "    #     )\n",
    "    # else:\n",
    "    #     tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    # Below is an example function to build the dataset. In our case, we use the IMDB dataset\n",
    "    # from the `datasets` library. One should customize this function to train the model on\n",
    "    # its own dataset.\n",
    "    def build_dataset(\n",
    "        tokenizer, dataset_name=\"lvwerra/stack-exchange-paired\", input_min_text_length=2, input_max_text_length=8\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build dataset for training. This builds the dataset from `load_dataset`, one should\n",
    "        customize this function to train the model on its own dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_name (`str`):\n",
    "                The name of the dataset to be loaded.\n",
    "\n",
    "        Returns:\n",
    "            dataloader (`torch.utils.data.DataLoader`):\n",
    "                The dataloader for the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        # load imdb with datasets\n",
    "        # ds = load_dataset(dataset_name, data_dir=\"data/rl\", split=\"train\")\n",
    "        ds = load_dataset(dataset_name, split=\"train\")\n",
    "        original_columns = ds.column_names\n",
    "        num_proc = 1 #24\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            new_examples = {\n",
    "                \"query\": [],\n",
    "                \"input_ids\": [],\n",
    "            }\n",
    "            # for question in examples[\"question\"]:\n",
    "            for question in examples[\"user_input\"]:\n",
    "                query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "                tokenized_question = tokenizer(query, truncation=True)\n",
    "                new_examples[\"query\"].append(query)\n",
    "                new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"])\n",
    "\n",
    "            return new_examples\n",
    "\n",
    "        ds = train_dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=num_proc,\n",
    "            remove_columns=original_columns,\n",
    "        )\n",
    "        # ds = ds.filter(lambda x: len(x[\"input_ids\"]) < 512, batched=False)\n",
    "\n",
    "        ds.set_format(type=\"torch\")\n",
    "        return ds\n",
    "\n",
    "\n",
    "    # We retrieve the dataloader by calling the `build_dataset` function.\n",
    "    # dataset = build_dataset(tokenizer)\n",
    "    dataset = build_dataset(tokenizer, dataset_name=dataset_name)\n",
    "\n",
    "\n",
    "    def collator(data):\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "    # set seed before initializing value head for deterministic eval\n",
    "    torch.manual_seed(config.seed)\n",
    "\n",
    "    # Now let's build the model, the reference model, and the tokenizer.\n",
    "    current_device = Accelerator().local_process_index\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=8, #16,\n",
    "        lora_alpha=16, #32,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_name,\n",
    "        # device_map={\"\": current_device},\n",
    "        peft_config=lora_config,\n",
    "        # layer_norm_names=[],\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    # Tạo generation_config nếu không có\n",
    "    if not hasattr(model, \"generation_config\"):\n",
    "        model.generation_config = GenerationConfig.from_pretrained(\"lmsys/vicuna-7b-v1.5-16k\")  # Dùng mô hình gốc\n",
    "\n",
    "    print(\"finetune model: \", model_name, type(model))\n",
    "    # print(\"finetune model's is_loaded_in_4bit: \", model.is_loaded_in_4bit)\n",
    "\n",
    "    print(\"reward_model_name: \", reward_model_name)\n",
    "    #! my self code to try peft reward model\n",
    "    reward_model = LlamaForSequenceClassification.from_pretrained(\n",
    "        reward_model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    print(\"reward_model: \", type(reward_model))\n",
    "    # print(\"reward_model is_loaded_in_4bit: \", reward_model.is_loaded_in_4bit)\n",
    "\n",
    "    # reward_model = prepare_model_for_int8_training(reward_model)\n",
    "    reward_model_config = LlamaConfig.from_pretrained(reward_model_name)\n",
    "\n",
    "    # Tải mô hình gốc cho value_model\n",
    "    value_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        \n",
    "    )\n",
    "\n",
    "    print(\"Model class:\", type(model))\n",
    "    print(\"Reward model class:\", type(reward_model))\n",
    "\n",
    "    optimizer = None\n",
    "    if script_args.adafactor:\n",
    "        optimizer = Adafactor(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            scale_parameter=False,\n",
    "            relative_step=False,\n",
    "            warmup_init=False,\n",
    "            lr=config.learning_rate,\n",
    "        )\n",
    "    # We then build the PPOTrainer, passing the model, the reference model, the tokenizer\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config,\n",
    "        processing_class=tokenizer,\n",
    "        model= model,\n",
    "        reward_model = reward_model,\n",
    "        ref_model=None,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=collator,\n",
    "        optimizers=(optimizer, None),\n",
    "        value_model=value_model,\n",
    "    )\n",
    "\n",
    "    # We then build the sentiment analysis pipeline, passing the model name and the\n",
    "    # sentiment analysis pipeline arguments. Let's also make sure to set the device\n",
    "    # to the same device as the PPOTrainer.\n",
    "    device = ppo_trainer.accelerator.device\n",
    "    if ppo_trainer.accelerator.num_processes == 1:\n",
    "        device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a ` pipeline` bug\n",
    "    print(\"device: \", device)\n",
    "\n",
    "\n",
    "    sentiment_pipe = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=reward_model_name,\n",
    "        # model=reward_model,\n",
    "        config=reward_model_config,\n",
    "        # TypeError: LlamaForSequenceClassification.__init__() got an unexpected keyword argument 'peft_config'\n",
    "        tokenizer=tokenizer,\n",
    "        # torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "    # We then define the arguments to pass to the `generate` function. These arguments\n",
    "    # are passed to the `generate` function of the PPOTrainer, which is a wrapper around\n",
    "    # the `generate` function of the trained model.\n",
    "    generation_kwargs = {\n",
    "        # \"min_length\": -1,\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": 100_000,\n",
    "    }\n",
    "    output_min_length = 32\n",
    "    output_max_length = script_args.output_max_length\n",
    "    output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            question_tensors,\n",
    "            return_prompt=False,\n",
    "            length_sampler=output_length_sampler,\n",
    "            **generation_kwargs,\n",
    "        )\n",
    "        batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "        # Compute sentiment score\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "        rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n",
    "\n",
    "        # Run PPO step\n",
    "        stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "        if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:\n",
    "            ppo_trainer.save_pretrained(script_args.output_dir + f\"step_{epoch}\")\n",
    "\n",
    "        ppo_trainer.save_pretrained(script_args.output_dir + \"step_saved\")\n",
    "\n",
    "\n",
    "# Optimize using reinforcement learning\n",
    "tuning_lm_with_rl(args)\n",
    "\n",
    "from predict_module.merge_peft_adapter import merge_peft_adapter\n",
    "merge_peft_adapter(model_name=args.output_dir+\"step_saved\", output_name=\"./saved_models/sep_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class PPOTrainer in module trl.trainer.ppo_trainer:\n",
      "\n",
      "class PPOTrainer(transformers.trainer.Trainer)\n",
      " |  PPOTrainer(args: trl.trainer.ppo_config.PPOConfig, processing_class: Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, transformers.image_processing_utils.BaseImageProcessor, transformers.feature_extraction_utils.FeatureExtractionMixin, transformers.processing_utils.ProcessorMixin, NoneType], model: torch.nn.modules.module.Module, ref_model: Optional[torch.nn.modules.module.Module], reward_model: torch.nn.modules.module.Module, train_dataset: datasets.arrow_dataset.Dataset, value_model: Optional[torch.nn.modules.module.Module] = None, data_collator: Optional[transformers.data.data_collator.DataCollatorWithPadding] = None, eval_dataset: Union[datasets.arrow_dataset.Dataset, dict[str, datasets.arrow_dataset.Dataset], NoneType] = None, optimizers: tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None), callbacks: Optional[list[transformers.trainer_callback.TrainerCallback]] = None, peft_config: Optional[ForwardRef('PeftConfig')] = None) -> None\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      PPOTrainer\n",
      " |      transformers.trainer.Trainer\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, args: trl.trainer.ppo_config.PPOConfig, processing_class: Union[transformers.tokenization_utils_base.PreTrainedTokenizerBase, transformers.image_processing_utils.BaseImageProcessor, transformers.feature_extraction_utils.FeatureExtractionMixin, transformers.processing_utils.ProcessorMixin, NoneType], model: torch.nn.modules.module.Module, ref_model: Optional[torch.nn.modules.module.Module], reward_model: torch.nn.modules.module.Module, train_dataset: datasets.arrow_dataset.Dataset, value_model: Optional[torch.nn.modules.module.Module] = None, data_collator: Optional[transformers.data.data_collator.DataCollatorWithPadding] = None, eval_dataset: Union[datasets.arrow_dataset.Dataset, dict[str, datasets.arrow_dataset.Dataset], NoneType] = None, optimizers: tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None), callbacks: Optional[list[transformers.trainer_callback.TrainerCallback]] = None, peft_config: Optional[ForwardRef('PeftConfig')] = None) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  create_model_card(self, model_name: Optional[str] = None, dataset_name: Optional[str] = None, tags: Union[str, list[str], NoneType] = None)\n",
      " |      Creates a draft of a model card using the information available to the `Trainer`.\n",
      " |\n",
      " |      Args:\n",
      " |          model_name (`str` or `None`, *optional*, defaults to `None`):\n",
      " |              Name of the model.\n",
      " |          dataset_name (`str` or `None`, *optional*, defaults to `None`):\n",
      " |              Name of the dataset used for training.\n",
      " |          tags (`str`, `list[str]` or `None`, *optional*, defaults to `None`):\n",
      " |              Tags to be associated with the model card.\n",
      " |\n",
      " |  generate_completions(self, sampling: bool = False)\n",
      " |\n",
      " |  get_eval_dataloader(self) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the evaluation [`~torch.utils.data.DataLoader`].\n",
      " |\n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |\n",
      " |      Args:\n",
      " |          eval_dataset (`str` or `torch.utils.data.Dataset`, *optional*):\n",
      " |              If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\n",
      " |\n",
      " |  get_train_dataloader(self) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the training [`~torch.utils.data.DataLoader`].\n",
      " |\n",
      " |      Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
      " |      training if necessary) otherwise.\n",
      " |\n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |\n",
      " |  null_ref_context(self)\n",
      " |      Context manager for handling null reference model (that is, peft adapter manipulation).\n",
      " |\n",
      " |  save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False)\n",
      " |      Will save the model, so you can reload it using `from_pretrained()`.\n",
      " |\n",
      " |      Will only save from the main process.\n",
      " |\n",
      " |  train(self)\n",
      " |      Main training entry point.\n",
      " |\n",
      " |      Args:\n",
      " |          resume_from_checkpoint (`str` or `bool`, *optional*):\n",
      " |              If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n",
      " |              `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n",
      " |              of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
      " |          trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n",
      " |              The trial run or the hyperparameter dictionary for hyperparameter search.\n",
      " |          ignore_keys_for_eval (`List[str]`, *optional*)\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions for evaluation during the training.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments used to hide deprecated arguments\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from transformers.trainer.Trainer:\n",
      " |\n",
      " |  add_callback(self, callback)\n",
      " |      Add a callback to the current list of [`~transformers.TrainerCallback`].\n",
      " |\n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformers.TrainerCallback]`):\n",
      " |             A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the\n",
      " |             first case, will instantiate a member of that class.\n",
      " |\n",
      " |  autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True)\n",
      " |      A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n",
      " |      arguments, depending on the situation.\n",
      " |\n",
      " |  call_model_init(self, trial=None)\n",
      " |\n",
      " |  compare_trainer_and_checkpoint_args(self, training_args, trainer_state)\n",
      " |\n",
      " |  compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None)\n",
      " |      How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
      " |\n",
      " |      Subclass and override for custom behavior.\n",
      " |\n",
      " |  compute_loss_context_manager(self)\n",
      " |      A helper wrapper to group together context managers.\n",
      " |\n",
      " |  create_accelerator_and_postprocess(self)\n",
      " |\n",
      " |  create_optimizer(self)\n",
      " |      Setup the optimizer.\n",
      " |\n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
      " |\n",
      " |  create_optimizer_and_scheduler(self, num_training_steps: int)\n",
      " |      Setup the optimizer and the learning rate scheduler.\n",
      " |\n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n",
      " |      `create_scheduler`) in a subclass.\n",
      " |\n",
      " |  create_scheduler(self, num_training_steps: int, optimizer: torch.optim.optimizer.Optimizer = None)\n",
      " |      Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n",
      " |      passed as an argument.\n",
      " |\n",
      " |      Args:\n",
      " |          num_training_steps (int): The number of training steps to do.\n",
      " |\n",
      " |  evaluate(self, eval_dataset: Union[torch.utils.data.dataset.Dataset, Dict[str, torch.utils.data.dataset.Dataset], NoneType] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> Dict[str, float]\n",
      " |      Run evaluation and returns metrics.\n",
      " |\n",
      " |      The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
      " |      (pass it to the init `compute_metrics` argument).\n",
      " |\n",
      " |      You can also subclass and override this method to inject custom behavior.\n",
      " |\n",
      " |      Args:\n",
      " |          eval_dataset (Union[`Dataset`, Dict[str, `Dataset`]), *optional*):\n",
      " |              Pass a dataset if you wish to override `self.eval_dataset`. If it is a [`~datasets.Dataset`], columns\n",
      " |              not accepted by the `model.forward()` method are automatically removed. If it is a dictionary, it will\n",
      " |              evaluate on each dataset, prepending the dictionary key to the metric name. Datasets must implement the\n",
      " |              `__len__` method.\n",
      " |\n",
      " |              <Tip>\n",
      " |\n",
      " |              If you pass a dictionary with names of datasets as keys and datasets as values, evaluate will run\n",
      " |              separate evaluations on each dataset. This can be useful to monitor how training affects other\n",
      " |              datasets or simply to get a more fine-grained evaluation.\n",
      " |              When used with `load_best_model_at_end`, make sure `metric_for_best_model` references exactly one\n",
      " |              of the datasets. If you, for example, pass in `{\"data1\": data1, \"data2\": data2}` for two datasets\n",
      " |              `data1` and `data2`, you could specify `metric_for_best_model=\"eval_data1_loss\"` for using the\n",
      " |              loss on `data1` and `metric_for_best_model=\"eval_data2_loss\"` for the loss on `data2`.\n",
      " |\n",
      " |              </Tip>\n",
      " |\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"eval_bleu\" if the prefix is \"eval\" (default)\n",
      " |\n",
      " |      Returns:\n",
      " |          A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
      " |          dictionary also contains the epoch number which comes from the training state.\n",
      " |\n",
      " |  evaluation_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |\n",
      " |      Works both with or without labels.\n",
      " |\n",
      " |  floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]])\n",
      " |      For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\n",
      " |      operations for every backward + forward pass. If using another model, either implement such a method in the\n",
      " |      model or subclass and override this method.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |\n",
      " |      Returns:\n",
      " |          `int`: The number of floating-point operations.\n",
      " |\n",
      " |  get_batch_samples(self, epoch_iterator, num_batches)\n",
      " |\n",
      " |  get_decay_parameter_names(self, model) -> List[str]\n",
      " |      Get all parameter names that weight decay will be applied to.\n",
      " |\n",
      " |      This function filters out parameters in two ways:\n",
      " |      1. By layer type (instances of layers specified in ALL_LAYERNORM_LAYERS)\n",
      " |      2. By parameter name patterns (containing 'bias', 'layernorm', or 'rmsnorm')\n",
      " |\n",
      " |  get_learning_rates(self)\n",
      " |      Returns the learning rate of each parameter from self.optimizer.\n",
      " |\n",
      " |  get_num_trainable_parameters(self)\n",
      " |      Get the number of trainable parameters.\n",
      " |\n",
      " |  get_optimizer_group(self, param: Union[str, torch.nn.parameter.Parameter, NoneType] = None)\n",
      " |      Returns optimizer group for a parameter if given, else returns all optimizer groups for params.\n",
      " |\n",
      " |      Args:\n",
      " |          param (`str` or `torch.nn.parameter.Parameter`, *optional*):\n",
      " |              The parameter for which optimizer group needs to be returned.\n",
      " |\n",
      " |  get_test_dataloader(self, test_dataset: torch.utils.data.dataset.Dataset) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the test [`~torch.utils.data.DataLoader`].\n",
      " |\n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |\n",
      " |      Args:\n",
      " |          test_dataset (`torch.utils.data.Dataset`, *optional*):\n",
      " |              The test dataset to use. If it is a [`~datasets.Dataset`], columns not accepted by the\n",
      " |              `model.forward()` method are automatically removed. It must implement `__len__`.\n",
      " |\n",
      " |  hyperparameter_search(self, hp_space: Optional[Callable[[ForwardRef('optuna.Trial')], Dict[str, float]]] = None, compute_objective: Optional[Callable[[Dict[str, float]], float]] = None, n_trials: int = 20, direction: Union[str, List[str]] = 'minimize', backend: Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None, hp_name: Optional[Callable[[ForwardRef('optuna.Trial')], str]] = None, **kwargs) -> Union[transformers.trainer_utils.BestRun, List[transformers.trainer_utils.BestRun]]\n",
      " |      Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\n",
      " |      by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n",
      " |      the sum of all metrics otherwise.\n",
      " |\n",
      " |      <Tip warning={true}>\n",
      " |\n",
      " |      To use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\n",
      " |      reinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\n",
      " |      subclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\n",
      " |      optimizer/scheduler.\n",
      " |\n",
      " |      </Tip>\n",
      " |\n",
      " |      Args:\n",
      " |          hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n",
      " |              A function that defines the hyperparameter search space. Will default to\n",
      " |              [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n",
      " |              [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n",
      " |          compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n",
      " |              A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n",
      " |              method. Will default to [`~trainer_utils.default_compute_objective`].\n",
      " |          n_trials (`int`, *optional*, defaults to 100):\n",
      " |              The number of trial runs to test.\n",
      " |          direction (`str` or `List[str]`, *optional*, defaults to `\"minimize\"`):\n",
      " |              If it's single objective optimization, direction is `str`, can be `\"minimize\"` or `\"maximize\"`, you\n",
      " |              should pick `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or\n",
      " |              several metrics. If it's multi objectives optimization, direction is `List[str]`, can be List of\n",
      " |              `\"minimize\"` and `\"maximize\"`, you should pick `\"minimize\"` when optimizing the validation loss,\n",
      " |              `\"maximize\"` when optimizing one or several metrics.\n",
      " |          backend (`str` or [`~training_utils.HPSearchBackend`], *optional*):\n",
      " |              The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n",
      " |              on which one is installed. If all are installed, will default to optuna.\n",
      " |          hp_name (`Callable[[\"optuna.Trial\"], str]]`, *optional*):\n",
      " |              A function that defines the trial/run name. Will default to None.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments for each backend:\n",
      " |\n",
      " |              - `optuna`: parameters from\n",
      " |                [optuna.study.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n",
      " |                and also the parameters `timeout`, `n_jobs` and `gc_after_trial` from\n",
      " |                [optuna.study.Study.optimize](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.Study.html#optuna.study.Study.optimize)\n",
      " |              - `ray`: parameters from [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run).\n",
      " |                If `resources_per_trial` is not set in the `kwargs`, it defaults to 1 CPU core and 1 GPU (if available).\n",
      " |                If `progress_reporter` is not set in the `kwargs`,\n",
      " |                [ray.tune.CLIReporter](https://docs.ray.io/en/latest/tune/api/doc/ray.tune.CLIReporter.html) is used.\n",
      " |              - `sigopt`: the parameter `proxies` from\n",
      " |                [sigopt.Connection.set_proxies](https://docs.sigopt.com/support/faq#how-do-i-use-sigopt-with-a-proxy).\n",
      " |\n",
      " |      Returns:\n",
      " |          [`trainer_utils.BestRun` or `List[trainer_utils.BestRun]`]: All the information about the best run or best\n",
      " |          runs for multi-objective optimization. Experiment summary can be found in `run_summary` attribute for Ray\n",
      " |          backend.\n",
      " |\n",
      " |  init_hf_repo(self, token: Optional[str] = None)\n",
      " |      Initializes a git repo in `self.args.hub_model_id`.\n",
      " |\n",
      " |  ipex_optimize_model(self, model, training=False, dtype=torch.float32)\n",
      " |\n",
      " |  is_local_process_zero(self) -> bool\n",
      " |      Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\n",
      " |      machines) main process.\n",
      " |\n",
      " |  is_world_process_zero(self) -> bool\n",
      " |      Whether or not this process is the global main process (when training in a distributed fashion on several\n",
      " |      machines, this is only going to be `True` for one process).\n",
      " |\n",
      " |  log(self, logs: Dict[str, float], start_time: Optional[float] = None) -> None\n",
      " |      Log `logs` on the various objects watching training.\n",
      " |\n",
      " |      Subclass and override this method to inject custom behavior.\n",
      " |\n",
      " |      Args:\n",
      " |          logs (`Dict[str, float]`):\n",
      " |              The values to log.\n",
      " |          start_time (`Optional[float]`):\n",
      " |              The start of training.\n",
      " |\n",
      " |  log_metrics(self, split, metrics) from transformers.trainer_pt_utils\n",
      " |      Log metrics in a specially formatted way\n",
      " |\n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |\n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predictmetrics: metrics dict\n",
      " |\n",
      " |      Notes on memory reports:\n",
      " |\n",
      " |      In order to get memory usage report you need to install `psutil`. You can do that with `pip install psutil`.\n",
      " |\n",
      " |      Now when this method is run, you will see a report that will include: :\n",
      " |\n",
      " |      ```\n",
      " |      init_mem_cpu_alloc_delta   =     1301MB\n",
      " |      init_mem_cpu_peaked_delta  =      154MB\n",
      " |      init_mem_gpu_alloc_delta   =      230MB\n",
      " |      init_mem_gpu_peaked_delta  =        0MB\n",
      " |      train_mem_cpu_alloc_delta  =     1345MB\n",
      " |      train_mem_cpu_peaked_delta =        0MB\n",
      " |      train_mem_gpu_alloc_delta  =      693MB\n",
      " |      train_mem_gpu_peaked_delta =        7MB\n",
      " |      ```\n",
      " |\n",
      " |      **Understanding the reports:**\n",
      " |\n",
      " |      - the first segment, e.g., `train__`, tells you which stage the metrics are for. Reports starting with `init_`\n",
      " |          will be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the\n",
      " |          `__init__` will be reported along with the `eval_` metrics.\n",
      " |      - the third segment, is either `cpu` or `gpu`, tells you whether it's the general RAM or the gpu0 memory\n",
      " |          metric.\n",
      " |      - `*_alloc_delta` - is the difference in the used/allocated memory counter between the end and the start of the\n",
      " |          stage - it can be negative if a function released more memory than it allocated.\n",
      " |      - `*_peaked_delta` - is any extra memory that was consumed and then freed - relative to the current allocated\n",
      " |          memory counter - it is never negative. When you look at the metrics of any stage you add up `alloc_delta` +\n",
      " |          `peaked_delta` and you know how much memory was needed to complete that stage.\n",
      " |\n",
      " |      The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the\n",
      " |      main process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may\n",
      " |      use a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more\n",
      " |      memory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the\n",
      " |      future these reports will evolve to measure those too.\n",
      " |\n",
      " |      The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the\n",
      " |      memory shared with other processes. It is important to note that it does not include swapped out memory, so the\n",
      " |      reports could be imprecise.\n",
      " |\n",
      " |      The CPU peak memory is measured using a sampling thread. Due to python's GIL it may miss some of the peak memory if\n",
      " |      that thread didn't get a chance to run when the highest memory was used. Therefore this report can be less than\n",
      " |      reality. Using `tracemalloc` would have reported the exact peak memory, but it doesn't report memory allocations\n",
      " |      outside of python. So if some C++ CUDA extension allocated its own memory it won't be reported. And therefore it\n",
      " |      was dropped in favor of the memory sampling approach, which reads the current process memory usage.\n",
      " |\n",
      " |      The GPU allocated and peak memory reporting is done with `torch.cuda.memory_allocated()` and\n",
      " |      `torch.cuda.max_memory_allocated()`. This metric reports only \"deltas\" for pytorch-specific allocations, as\n",
      " |      `torch.cuda` memory management system doesn't track any memory allocated outside of pytorch. For example, the very\n",
      " |      first cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.\n",
      " |\n",
      " |      Note that this tracker doesn't account for memory allocations outside of [`Trainer`]'s `__init__`, `train`,\n",
      " |      `evaluate` and `predict` calls.\n",
      " |\n",
      " |      Because `evaluation` calls may happen during `train`, we can't handle nested invocations because\n",
      " |      `torch.cuda.max_memory_allocated` is a single counter, so if it gets reset by a nested eval call, `train`'s tracker\n",
      " |      will report incorrect info. If this [pytorch issue](https://github.com/pytorch/pytorch/issues/16266) gets resolved\n",
      " |      it will be possible to change this class to be re-entrant. Until then we will only track the outer level of\n",
      " |      `train`, `evaluate` and `predict` methods. Which means that if `eval` is called during `train`, it's the latter\n",
      " |      that will account for its memory usage and that of the former.\n",
      " |\n",
      " |      This also means that if any other tool that is used along the [`Trainer`] calls\n",
      " |      `torch.cuda.reset_peak_memory_stats`, the gpu peak memory stats could be invalid. And the [`Trainer`] will disrupt\n",
      " |      the normal behavior of any such tools that rely on calling `torch.cuda.reset_peak_memory_stats` themselves.\n",
      " |\n",
      " |      For best performance you may want to consider turning the memory profiling off for production runs.\n",
      " |\n",
      " |  metrics_format(self, metrics: Dict[str, float]) -> Dict[str, float] from transformers.trainer_pt_utils\n",
      " |      Reformat Trainer metrics values to a human-readable format\n",
      " |\n",
      " |      Args:\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |\n",
      " |      Returns:\n",
      " |          metrics (`Dict[str, float]`): The reformatted metrics\n",
      " |\n",
      " |  num_examples(self, dataloader: torch.utils.data.dataloader.DataLoader) -> int\n",
      " |      Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset. When\n",
      " |      dataloader.dataset does not exist or has no length, estimates as best it can\n",
      " |\n",
      " |  pop_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformers.TrainerCallback`] and returns it.\n",
      " |\n",
      " |      If the callback is not found, returns `None` (and no error is raised).\n",
      " |\n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformers.TrainerCallback]`):\n",
      " |             A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the\n",
      " |             first case, will pop the first member of that class found in the list of callbacks.\n",
      " |\n",
      " |      Returns:\n",
      " |          [`~transformers.TrainerCallback`]: The callback removed, if found.\n",
      " |\n",
      " |  predict(self, test_dataset: torch.utils.data.dataset.Dataset, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'test') -> transformers.trainer_utils.PredictionOutput\n",
      " |      Run prediction and returns predictions and potential metrics.\n",
      " |\n",
      " |      Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n",
      " |      will also return metrics, like in `evaluate()`.\n",
      " |\n",
      " |      Args:\n",
      " |          test_dataset (`Dataset`):\n",
      " |              Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n",
      " |              `model.forward()` method are automatically removed. Has to implement the method `__len__`\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"test_bleu\" if the prefix is \"test\" (default)\n",
      " |\n",
      " |      <Tip>\n",
      " |\n",
      " |      If your predictions or labels have different sequence length (for instance because you're doing dynamic padding\n",
      " |      in a token classification task) the predictions will be padded (on the right) to allow for concatenation into\n",
      " |      one array. The padding index is -100.\n",
      " |\n",
      " |      </Tip>\n",
      " |\n",
      " |      Returns: *NamedTuple* A namedtuple with the following keys:\n",
      " |\n",
      " |          - predictions (`np.ndarray`): The predictions on `test_dataset`.\n",
      " |          - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n",
      " |          - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n",
      " |            labels).\n",
      " |\n",
      " |  prediction_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |\n",
      " |      Works both with or without labels.\n",
      " |\n",
      " |  prediction_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\n",
      " |      Perform an evaluation step on `model` using `inputs`.\n",
      " |\n",
      " |      Subclass and override to inject custom behavior.\n",
      " |\n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to evaluate.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |\n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |          prediction_loss_only (`bool`):\n",
      " |              Whether or not to return the loss only.\n",
      " |          ignore_keys (`List[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |\n",
      " |      Return:\n",
      " |          Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
      " |          logits and labels (each being optional).\n",
      " |\n",
      " |  propagate_args_to_deepspeed(self, auto_find_batch_size=False)\n",
      " |      Sets values in the deepspeed plugin based on the Trainer args\n",
      " |\n",
      " |  push_to_hub(self, commit_message: Optional[str] = 'End of training', blocking: bool = True, token: Optional[str] = None, revision: Optional[str] = None, **kwargs) -> str\n",
      " |      Upload `self.model` and `self.processing_class` to the 🤗 model hub on the repo `self.args.hub_model_id`.\n",
      " |\n",
      " |      Parameters:\n",
      " |          commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n",
      " |              Message to commit while pushing.\n",
      " |          blocking (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the function should return only when the `git push` has finished.\n",
      " |          token (`str`, *optional*, defaults to `None`):\n",
      " |              Token with write permission to overwrite Trainer's original args.\n",
      " |          revision (`str`, *optional*):\n",
      " |              The git revision to commit from. Defaults to the head of the \"main\" branch.\n",
      " |          kwargs (`Dict[str, Any]`, *optional*):\n",
      " |              Additional keyword arguments passed along to [`~Trainer.create_model_card`].\n",
      " |\n",
      " |      Returns:\n",
      " |          The URL of the repository where the model was pushed if `blocking=False`, or a `Future` object tracking the\n",
      " |          progress of the commit if `blocking=True`.\n",
      " |\n",
      " |  remove_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformers.TrainerCallback`].\n",
      " |\n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformers.TrainerCallback]`):\n",
      " |             A [`~transformers.TrainerCallback`] class or an instance of a [`~transformers.TrainerCallback`]. In the\n",
      " |             first case, will remove the first member of that class found in the list of callbacks.\n",
      " |\n",
      " |  save_metrics(self, split, metrics, combined=True) from transformers.trainer_pt_utils\n",
      " |      Save metrics into a json file for that split, e.g. `train_results.json`.\n",
      " |\n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |\n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`, `all`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |          combined (`bool`, *optional*, defaults to `True`):\n",
      " |              Creates combined metrics by updating `all_results.json` with metrics of this call\n",
      " |\n",
      " |      To understand the metrics please read the docstring of [`~Trainer.log_metrics`]. The only difference is that raw\n",
      " |      unformatted numbers are saved in the current method.\n",
      " |\n",
      " |  save_state(self) from transformers.trainer_pt_utils\n",
      " |      Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model\n",
      " |\n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |\n",
      " |  set_initial_training_values(self, args: transformers.training_args.TrainingArguments, dataloader: torch.utils.data.dataloader.DataLoader, total_train_batch_size: int)\n",
      " |      Calculates and returns the following values:\n",
      " |      - `num_train_epochs`\n",
      " |      - `num_update_steps_per_epoch`\n",
      " |      - `num_examples`\n",
      " |      - `num_train_samples`\n",
      " |      - `epoch_based`\n",
      " |      - `len_dataloader`\n",
      " |      - `max_steps`\n",
      " |\n",
      " |  store_flos(self)\n",
      " |\n",
      " |  torch_jit_model_eval(self, model, dataloader, training=False)\n",
      " |\n",
      " |  training_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]], num_items_in_batch=None) -> torch.Tensor\n",
      " |      Perform a training step on a batch of inputs.\n",
      " |\n",
      " |      Subclass and override to inject custom behavior.\n",
      " |\n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to train.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |\n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |\n",
      " |      Return:\n",
      " |          `torch.Tensor`: The tensor with training loss on this batch.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from transformers.trainer.Trainer:\n",
      " |\n",
      " |  get_optimizer_cls_and_kwargs(args: transformers.training_args.TrainingArguments, model: Optional[transformers.modeling_utils.PreTrainedModel] = None) -> Tuple[Any, Any]\n",
      " |      Returns the optimizer class and optimizer parameters based on the training arguments.\n",
      " |\n",
      " |      Args:\n",
      " |          args (`transformers.training_args.TrainingArguments`):\n",
      " |              The training arguments for the training session.\n",
      " |\n",
      " |  num_tokens(train_dl: torch.utils.data.dataloader.DataLoader, max_steps: Optional[int] = None) -> int\n",
      " |      Helper to get number of tokens in a [`~torch.utils.data.DataLoader`] by enumerating dataloader.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from transformers.trainer.Trainer:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  tokenizer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from trl import PPOConfig\n",
    "# help(PPOConfig)\n",
    "help(PPOTrainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\2\\ipykernel_7488\\3367921793.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{model_name}/pytorch_model.bin\", map_location=\"cpu\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './saved_models/lora-Vicuna-adapter-merged/pytorch_model.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m model_name=args.rl_base_model\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m state_dict = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/pytorch_model.bin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(state_dict[\u001b[33m\"\u001b[39m\u001b[33mmodel.layers.0.self_attn.q_proj.weight\u001b[39m\u001b[33m\"\u001b[39m].shape)  \u001b[38;5;66;03m# Kiểm tra shape\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './saved_models/lora-Vicuna-adapter-merged/pytorch_model.bin'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model_name=args.rl_base_model\n",
    "\n",
    "state_dict = torch.load(f\"{model_name}/pytorch_model.bin\", map_location=\"cpu\")\n",
    "print(state_dict[\"model.layers.0.self_attn.q_proj.weight\"].shape)  # Kiểm tra shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
