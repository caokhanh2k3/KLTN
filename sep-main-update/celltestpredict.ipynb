{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.26.4\n",
    "# !pip install openai==1.12.0\n",
    "# !pip install tenacity==8.2.3\n",
    "# !pip install tiktoken==0.6.0\n",
    "# # !pip install transformers==4.34.1\n",
    "# !pip install pandas==2.2.0\n",
    "# !pip install scikit-learn==1.4.0\n",
    "# # !pip install torch==2.2.0+cu118\n",
    "# # !pip install bitsandbytes==0.42.0\n",
    "# !pip install datasets==2.14.7\n",
    "# # !pip install sentencepiece==0.1.99\n",
    "# !pip install peft==0.6.2\n",
    "# !pip install evaluate==0.4.1\n",
    "# !pip install trl==0.7.1\n",
    "# !pip install protobuf==4.25.2\n",
    "# !pip install python-dotenv\n",
    "# !pip install pandas_ta\n",
    "# !pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft==0.6.2\n",
      "  Using cached peft-0.6.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft==0.6.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from peft==0.6.2) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from peft==0.6.2) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft==0.6.2) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft==0.6.2) (2.2.0+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft==0.6.2) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft==0.6.2) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft==0.6.2) (1.4.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft==0.6.2) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=0.21.0->peft==0.6.2) (0.29.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft==0.6.2) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->peft==0.6.2) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->peft==0.6.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->peft==0.6.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->peft==0.6.2) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.13.0->peft==0.6.2) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->peft==0.6.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->peft==0.6.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->peft==0.6.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers->peft==0.6.2) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.13.0->peft==0.6.2) (1.3.0)\n",
      "Using cached peft-0.6.2-py3-none-any.whl (174 kB)\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.14.0\n",
      "    Uninstalling peft-0.14.0:\n",
      "      Successfully uninstalled peft-0.14.0\n",
      "Successfully installed peft-0.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install transformers==4.38.2\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers\n",
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.2.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install -U bitsandbytes\n",
    "# !pip install accelerate\n",
    "# !pip install peft==0.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.2.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "print(torch.cuda.is_available())  # Nếu trả về False, CUDA chưa hoạt động\n",
    "print(torch.cuda.device_count())  # Kiểm tra số lượng GPU\n",
    "print(torch.cuda.get_device_name(0))  # Hiển thị tên GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.6.2)\n",
      "Collecting peft\n",
      "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (2.2.0+cu121)\n",
      "Requirement already satisfied: transformers in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (4.49.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (1.4.0)\n",
      "Requirement already satisfied: safetensors in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.25.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from peft) (0.29.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\administrator\\appdata\\roaming\\python\\python312\\site-packages (from tqdm->peft) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers->peft) (0.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\administrator\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
      "Installing collected packages: peft\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.6.2\n",
      "    Uninstalling peft-0.6.2:\n",
      "      Successfully uninstalled peft-0.6.2\n",
      "Successfully installed peft-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: peft\n",
      "Version: 0.14.0\n",
      "Summary: Parameter-Efficient Fine-Tuning (PEFT)\n",
      "Home-page: https://github.com/huggingface/peft\n",
      "Author: The HuggingFace team\n",
      "Author-email: benjamin@huggingface.co\n",
      "License: Apache\n",
      "Location: C:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
      "Requires: accelerate, huggingface-hub, numpy, packaging, psutil, pyyaml, safetensors, torch, tqdm, transformers\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# !pip show transformers bitsandbytes torch\n",
    "# !pip install accelerate\n",
    "# !pip install transformers --upgrade\n",
    "!pip install peft --upgrade\n",
    "!pip show peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(price_dir='data/price/preprocessed/', tweet_dir='data/tweet/raw/', seq_len=5, wandb=False, data_path='./data/merge_sample.json', output_path='./saved_models/lora-Vicuna', model_path='lmsys/vicuna-7b-v1.5-16k', eval_steps=200, save_steps=200, resume_from_supervised_checkpoint=None, ignore_data_skip='False', num_reflect_trials=2, datasets_dir='./datasets/', local_rank=0, resume_from_reward_checkpoint=False, deepspeed=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, reward_gradient_accumulation_steps=32, reward_learning_rate=2e-05, weight_decay=0.001, reward_base_model='lmsys/vicuna-7b-v1.5-16k', bf16=False, num_train_epochs=1, train_subset=100000, eval_subset=50000, gradient_checkpointing=False, optim='adamw_hf', lr_scheduler_type='linear', reward_adapter='./saved_models/reward_model_vicuna-7b', rl_base_model='./saved_models/lora-Vicuna-adapter-merged', tokenizer_name='lmsys/vicuna-7b-v1.5-16k', reward_model_name='./saved_models/reward_model_vicuna-7b-adapter-merged', log_with=None, rl_learning_rate=1.4e-05, output_max_length=128, mini_batch_size=1, batch_size=1, ppo_epochs=4, rl_gradient_accumulation_steps=1, adafactor=False, early_stopping=True, target_kl=0.1, reward_baseline=0, batched_gen=True, save_freq=None, output_dir='./saved_models/tuning_llama_rl_checkpoints/', seed=0, num_shots=4, save_dir='results/')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/merge_sample.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "fix_seed = 100\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    price_dir=\"data/price/preprocessed/\",\n",
    "    tweet_dir=\"data/tweet/raw/\",\n",
    "    seq_len=5,\n",
    "    wandb=False,\n",
    "    data_path=\"./data/merge_sample.json\",\n",
    "    output_path=\"./saved_models/lora-Vicuna\",\n",
    "    model_path=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    resume_from_supervised_checkpoint=None,\n",
    "    ignore_data_skip=\"False\",\n",
    "    num_reflect_trials=2,\n",
    "    datasets_dir=\"./datasets/\",\n",
    "    local_rank=0,\n",
    "    resume_from_reward_checkpoint=False,\n",
    "    deepspeed=None,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    reward_gradient_accumulation_steps=32,\n",
    "    reward_learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    reward_base_model=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    bf16=False,\n",
    "    num_train_epochs=1,\n",
    "    train_subset=100000,\n",
    "    eval_subset=50000,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_hf\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    reward_adapter=\"./saved_models/reward_model_vicuna-7b\",\n",
    "    rl_base_model=\"./saved_models/lora-Vicuna-adapter-merged\",\n",
    "    tokenizer_name=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    reward_model_name=\"./saved_models/reward_model_vicuna-7b-adapter-merged\",\n",
    "    log_with=None,\n",
    "    rl_learning_rate=1.4e-5,\n",
    "    output_max_length=128,\n",
    "    mini_batch_size=1,\n",
    "    batch_size=1,\n",
    "    ppo_epochs=4,\n",
    "    rl_gradient_accumulation_steps=1,\n",
    "    adafactor=False,\n",
    "    early_stopping=True,\n",
    "    target_kl=0.1,\n",
    "    reward_baseline=0,\n",
    "    batched_gen=True,\n",
    "    save_freq=None,\n",
    "    output_dir=\"./saved_models/tuning_llama_rl_checkpoints/\",\n",
    "    seed=0,\n",
    "    num_shots=4,\n",
    "    save_dir=\"results/\"\n",
    ")\n",
    "\n",
    "print(\"Args in experiment:\")\n",
    "print(args)\n",
    "\n",
    "args.data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: lmsys/vicuna-7b-v1.5-16k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  3.37it/s]\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:629: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:634: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\other.py:136: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 18.95 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 160\u001b[39m\n\u001b[32m    133\u001b[39m     model = prepare_model_for_int8_training(model)\n\u001b[32m    135\u001b[39m     \u001b[38;5;66;03m# # Chuyển mô hình sang lượng tử hóa thủ công (yêu cầu bitsandbytes hỗ trợ)\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;66;03m# from transformers import HfQuantizer\u001b[39;00m\n\u001b[32m    137\u001b[39m     \u001b[38;5;66;03m# quantizer = HfQuantizer.from_config(quant_config, model.config)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    158\u001b[39m \n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# Train supervised policy\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m \u001b[43msupervised_finetune\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;66;03m# merge_peft_adapter(model_name=args.output_path, output_name=args.rl_base_model)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36msupervised_finetune\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    123\u001b[39m model = LlamaForCausalLM.from_pretrained(\n\u001b[32m    124\u001b[39m     args.model_path,\n\u001b[32m    125\u001b[39m     quantization_config=quant_config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# Không thêm device_map hoặc bất kỳ tham số nào có thể kích hoạt accelerate\u001b[39;00m\n\u001b[32m    130\u001b[39m )\n\u001b[32m    132\u001b[39m \u001b[38;5;66;03m# check_quantization(model)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m model = \u001b[43mprepare_model_for_int8_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\other.py:140\u001b[39m, in \u001b[36mprepare_model_for_int8_training\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_model_for_int8_training\u001b[39m(*args, **kwargs):\n\u001b[32m    136\u001b[39m     warnings.warn(\n\u001b[32m    137\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mprepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    138\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    139\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprepare_model_for_kbit_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\peft\\utils\\other.py:98\u001b[39m, in \u001b[36mprepare_model_for_kbit_training\u001b[39m\u001b[34m(model, use_gradient_checkpointing, gradient_checkpointing_kwargs)\u001b[39m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.parameters():\n\u001b[32m     97\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (param.dtype == torch.float16) \u001b[38;5;129;01mor\u001b[39;00m (param.dtype == torch.bfloat16):\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m             param.data = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (loaded_in_kbit \u001b[38;5;129;01mor\u001b[39;00m is_gptq_quantized) \u001b[38;5;129;01mand\u001b[39;00m use_gradient_checkpointing:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# When having `use_reentrant=False` + gradient_checkpointing, there is no need for this hack\u001b[39;00m\n\u001b[32m    102\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gradient_checkpointing_kwargs \u001b[38;5;129;01mor\u001b[39;00m gradient_checkpointing_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_reentrant\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    103\u001b[39m         \u001b[38;5;66;03m# For backward compatibility\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 172.00 MiB. GPU 0 has a total capacity of 24.00 GiB of which 0 bytes is free. Of the allocated memory 18.95 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import bitsandbytes as bnb\n",
    "import transformers\n",
    "import argparse\n",
    "import warnings\n",
    "\n",
    "from datasets import load_dataset\n",
    "from predict_module import sft_dataloader\n",
    "\n",
    "# def supervised_finetune(args):\n",
    "#     MICRO_BATCH_SIZE = 4\n",
    "#     BATCH_SIZE = 128\n",
    "#     MAX_STEPS = None\n",
    "#     GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "#     EPOCHS = 2\n",
    "#     LEARNING_RATE = 3e-4\n",
    "#     CUTOFF_LEN = 256\n",
    "#     LORA_R = 8\n",
    "#     LORA_ALPHA = 16\n",
    "#     LORA_DROPOUT = 0.05\n",
    "#     VAL_PCT = 0.1\n",
    "#     TARGET_MODULES = [\n",
    "#         \"q_proj\",\n",
    "#         \"v_proj\",\n",
    "#     ]\n",
    "#     DATA_PATH = args.data_path\n",
    "#     OUTPUT_DIR = args.output_path\n",
    "#     device_map = \"auto\"\n",
    "#     world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "#     ddp = world_size != 1\n",
    "#     # print(\"ddp: \", ddp, \"world_size: \", world_size)\n",
    "#     if ddp:\n",
    "#         device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "#         GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    "#     print(args.model_path)\n",
    "\n",
    "#     # Cấu hình lượng tử hóa 4-bit với BitsAndBytesConfig\n",
    "#     quant_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True,              # Kích hoạt lượng tử hóa 4-bit\n",
    "#         bnb_4bit_quant_type=\"nf4\",      # Loại lượng tử hóa (NF4 được khuyến nghị)\n",
    "#         bnb_4bit_compute_dtype=torch.float16  # Kiểu dữ liệu tính toán\n",
    "#     )\n",
    "\n",
    "#     # Tải mô hình trực tiếp với device_map, không cần init_empty_weights hoặc load_checkpoint_and_dispatch\n",
    "#     model = LlamaForCausalLM.from_pretrained(\n",
    "#         args.model_path,\n",
    "#         quantization_config=quant_config,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         # device_map=device_map,  # Phân phối tự động lên GPU/CPU\n",
    "#         _fast_init=True\n",
    "#     )\n",
    "\n",
    "#     # Tải tokenizer\n",
    "#     tokenizer = LlamaTokenizer.from_pretrained(\n",
    "#         args.model_path,\n",
    "#         add_eos_token=True,\n",
    "#         local_files_only=args.offline if hasattr(args, 'offline') else False\n",
    "#     )\n",
    "\n",
    "def check_quantization(model):\n",
    "    if hasattr(model, \"quantization_method\"):\n",
    "        print(f\"Mô hình đã được lượng tử hóa với phương pháp: {model.quantization_method}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Mô hình không được lượng tử hóa.\")\n",
    "        return False\n",
    "\n",
    "def supervised_finetune(args):\n",
    "    # Các hằng số huấn luyện\n",
    "    MICRO_BATCH_SIZE = 4\n",
    "    BATCH_SIZE = 128\n",
    "    MAX_STEPS = None\n",
    "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "    EPOCHS = 2\n",
    "    LEARNING_RATE = 3e-4\n",
    "    CUTOFF_LEN = 256\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    VAL_PCT = 0.1\n",
    "    TARGET_MODULES = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ]\n",
    "    DATA_PATH = args.data_path\n",
    "    OUTPUT_DIR = args.output_path\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "\n",
    "    # Xử lý phân tán (Distributed Data Parallel - DDP)\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        torch.cuda.set_device(int(os.environ.get(\"LOCAL_RANK\", 0)))\n",
    "        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    "\n",
    "    # In đường dẫn mô hình để kiểm tra\n",
    "    print(f\"Loading model from: {args.model_path}\")\n",
    "\n",
    "    # Kiểm tra GPU có sẵn\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available. This script requires a GPU with bitsandbytes support.\")\n",
    "\n",
    "    # Cấu hình lượng tử hóa 4-bit với BitsAndBytesConfig\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # Tải mô hình mà không dùng device_map hoặc bất kỳ cơ chế phân phối nào từ accelerate\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        quantization_config=quant_config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=None,\n",
    "        low_cpu_mem_usage=False,  # Tắt cơ chế tự động phân phối của accelerate\n",
    "        # Không thêm device_map hoặc bất kỳ tham số nào có thể kích hoạt accelerate\n",
    "    )\n",
    "\n",
    "    # check_quantization(model)\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    # Tải tokenizer\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        args.model_path,\n",
    "        add_eos_token=True,\n",
    "        local_files_only=args.offline if hasattr(args, 'offline') else False\n",
    "    )\n",
    "\n",
    "    # Cấu hình LoRA cho fine-tuning\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Train supervised policy\n",
    "supervised_finetune(args)\n",
    "# merge_peft_adapter(model_name=args.output_path, output_name=args.rl_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_finetune(args):\n",
    "    MICRO_BATCH_SIZE = 4\n",
    "    BATCH_SIZE = 128\n",
    "    MAX_STEPS = None\n",
    "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "    EPOCHS = 2\n",
    "    LEARNING_RATE = 3e-4\n",
    "    CUTOFF_LEN = 256\n",
    "    LORA_R = 8\n",
    "    LORA_ALPHA = 16\n",
    "    LORA_DROPOUT = 0.05\n",
    "    VAL_PCT = 0.1\n",
    "    TARGET_MODULES = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ]\n",
    "    DATA_PATH = args.data_path\n",
    "    OUTPUT_DIR = args.output_path\n",
    "\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n",
    "    print(args.model_path)\n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        args.model_path,\n",
    "        load_in_4bit=True,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\n",
    "        args.model_path, add_eos_token=True\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "    # tokenizer.padding_side = \"left\"  # Allow batched inference\n",
    "\n",
    "    data = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "    val_set_size = VAL_PCT * len(data)\n",
    "\n",
    "    now_max_steps = max(\n",
    "        (len(data[\"train\"]) - val_set_size) // BATCH_SIZE * EPOCHS, EPOCHS)\n",
    "    if args.resume_from_supervised_checkpoint:\n",
    "        # Check the available weights and load them\n",
    "        checkpoint_name = os.path.join(\n",
    "            args.resume_from_supervised_checkpoint, \"pytorch_model.bin\"\n",
    "        )  # Full checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            pytorch_bin_path = checkpoint_name\n",
    "            checkpoint_name = os.path.join(\n",
    "                args.resume_from_supervised_checkpoint, \"adapter_model.bin\"\n",
    "            )  # only LoRA model - LoRA config above has to fit\n",
    "            if os.path.exists(checkpoint_name):\n",
    "                os.rename(checkpoint_name, pytorch_bin_path)\n",
    "                warnings.warn(\n",
    "                    \"The file name of the lora checkpoint'adapter_model.bin' is replaced with 'pytorch_model.bin'\")\n",
    "            else:\n",
    "                args.resume_from_supervised_checkpoint = (\n",
    "                    None  # So the trainer won't try loading its state\n",
    "                )\n",
    "        # The two files above have a different name depending on how they were saved, but are actually the same.\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Restarting from {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)\n",
    "        else:\n",
    "            print(f\"Checkpoint {checkpoint_name} not found\")\n",
    "\n",
    "        train_args_path = os.path.join(\n",
    "            resume_from_checkpoint, \"trainer_state.json\")\n",
    "\n",
    "        if os.path.exists(train_args_path):\n",
    "            import json\n",
    "            base_train_args = json.load(open(train_args_path, 'r'))\n",
    "            base_max_steps = base_train_args[\"max_steps\"]\n",
    "            resume_scale = base_max_steps / now_max_steps\n",
    "            if base_max_steps > now_max_steps:\n",
    "                warnings.warn(\"epoch {} replace to the base_max_steps {}\".format(\n",
    "                    EPOCHS, base_max_steps))\n",
    "                EPOCHS = None\n",
    "                MAX_STEPS = base_max_steps\n",
    "            else:\n",
    "                MAX_STEPS = now_max_steps\n",
    "    else:\n",
    "        MAX_STEPS = now_max_steps\n",
    "\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "    dataloader = sft_dataloader.SFTDataLoader(\n",
    "        data, CUTOFF_LEN, val_set_size, tokenizer)\n",
    "    train_data, val_data = dataloader.load_data()\n",
    "\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            max_steps=MAX_STEPS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            fp16=True,\n",
    "            logging_steps=20,\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=args.eval_steps if val_set_size > 0 else None,\n",
    "            save_steps=args.save_steps,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            save_total_limit=30,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            report_to=\"wandb\" if args.wandb else [],\n",
    "            ignore_data_skip=args.ignore_data_skip,\n",
    "        ),\n",
    "        data_collator=transformers.DataCollatorForLanguageModeling(\n",
    "            tokenizer, mlm=False)\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "\n",
    "    old_state_dict = model.state_dict\n",
    "    model.state_dict = (\n",
    "        lambda self, *_, **__: get_peft_model_state_dict(self, old_state_dict())\n",
    "    ).__get__(model, type(model))\n",
    "\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    print(\"\\n If there's a warning about missing keys above, please disregard :)\")\n",
    "\n",
    "    with torch.autocast(\"cuda\"):\n",
    "        trainer.train(resume_from_checkpoint=args.resume_from_supervised_checkpoint)\n",
    "\n",
    "    model.save_pretrained(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
