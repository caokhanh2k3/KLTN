{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(price_dir='data/price/preprocessed/', tweet_dir='data/tweet/raw/', seq_len=5, wandb=False, data_path='./data/merge_sample.json', output_path='./saved_models/lora-Vicuna', model_path='lmsys/vicuna-7b-v1.5-16k', eval_steps=200, save_steps=200, resume_from_supervised_checkpoint=None, ignore_data_skip='False', num_reflect_trials=2, datasets_dir='./datasets/', local_rank=0, resume_from_reward_checkpoint=False, deepspeed=None, per_device_train_batch_size=1, per_device_eval_batch_size=1, reward_gradient_accumulation_steps=32, reward_learning_rate=2e-05, weight_decay=0.001, reward_base_model='lmsys/vicuna-7b-v1.5-16k', bf16=False, num_train_epochs=1, train_subset=100000, eval_subset=50000, gradient_checkpointing=False, optim='adamw_hf', lr_scheduler_type='linear', reward_adapter='./saved_models/reward_model_vicuna-7b', rl_base_model='./saved_models/lora-Vicuna-adapter-merged', tokenizer_name='lmsys/vicuna-7b-v1.5-16k', reward_model_name='./saved_models/reward_model_vicuna-7b-adapter-merged', log_with=None, rl_learning_rate=1.4e-05, output_max_length=128, mini_batch_size=1, batch_size=1, ppo_epochs=4, rl_gradient_accumulation_steps=1, adafactor=False, early_stopping=True, target_kl=0.1, reward_baseline=0, batched_gen=True, save_freq=None, output_dir='./saved_models/tuning_llama_rl_checkpoints/', seed=0, num_shots=4, save_dir='results/')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/merge_sample.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "fix_seed = 100\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    price_dir=\"data/price/preprocessed/\",\n",
    "    tweet_dir=\"data/tweet/raw/\",\n",
    "    seq_len=5,\n",
    "    wandb=False,\n",
    "    data_path=\"./data/merge_sample.json\",\n",
    "    output_path=\"./saved_models/lora-Vicuna\",\n",
    "    model_path=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    resume_from_supervised_checkpoint=None,\n",
    "    ignore_data_skip=\"False\",\n",
    "    num_reflect_trials=2,\n",
    "    datasets_dir=\"./datasets/\",\n",
    "    local_rank=0,\n",
    "    resume_from_reward_checkpoint=False,\n",
    "    deepspeed=None,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    reward_gradient_accumulation_steps=32,\n",
    "    reward_learning_rate=2e-5,\n",
    "    weight_decay=0.001,\n",
    "    reward_base_model=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    bf16=False,\n",
    "    num_train_epochs=1,\n",
    "    train_subset=100000,\n",
    "    eval_subset=50000,\n",
    "    gradient_checkpointing=False,\n",
    "    optim=\"adamw_hf\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    reward_adapter=\"./saved_models/reward_model_vicuna-7b\",\n",
    "    rl_base_model=\"./saved_models/lora-Vicuna-adapter-merged\",\n",
    "    tokenizer_name=\"lmsys/vicuna-7b-v1.5-16k\",\n",
    "    reward_model_name=\"./saved_models/reward_model_vicuna-7b-adapter-merged\",\n",
    "    log_with=None,\n",
    "    rl_learning_rate=1.4e-5,\n",
    "    output_max_length=128,\n",
    "    mini_batch_size=1,\n",
    "    batch_size=1,\n",
    "    ppo_epochs=4,\n",
    "    rl_gradient_accumulation_steps=1,\n",
    "    adafactor=False,\n",
    "    early_stopping=True,\n",
    "    target_kl=0.1,\n",
    "    reward_baseline=0,\n",
    "    batched_gen=True,\n",
    "    save_freq=None,\n",
    "    output_dir=\"./saved_models/tuning_llama_rl_checkpoints/\",\n",
    "    seed=0,\n",
    "    num_shots=4,\n",
    "    save_dir=\"results/\"\n",
    ")\n",
    "\n",
    "print(\"Args in experiment:\")\n",
    "print(args)\n",
    "\n",
    "args.data_path\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
