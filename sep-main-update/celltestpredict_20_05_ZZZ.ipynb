{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl==0.11.4\n",
      "  Downloading trl-0.11.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in /root/.venv/lib/python3.10/site-packages (from trl==0.11.4) (2.5.0+cu124)\n",
      "Requirement already satisfied: transformers>=4.40.0 in /root/.venv/lib/python3.10/site-packages (from trl==0.11.4) (4.51.3)\n",
      "Requirement already satisfied: accelerate in /root/.venv/lib/python3.10/site-packages (from trl==0.11.4) (1.7.0)\n",
      "Requirement already satisfied: datasets in /root/.venv/lib/python3.10/site-packages (from trl==0.11.4) (3.6.0)\n",
      "Collecting tyro>=0.5.11 (from trl==0.11.4)\n",
      "  Downloading tyro-0.9.20-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /root/.venv/lib/python3.10/site-packages (from trl==0.11.4) (2.2.6)\n",
      "Requirement already satisfied: filelock in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (4.13.2)\n",
      "Requirement already satisfied: networkx in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /root/.venv/lib/python3.10/site-packages (from torch>=1.4.0->trl==0.11.4) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/.venv/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.4.0->trl==0.11.4) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (0.31.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (2024.11.6)\n",
      "Requirement already satisfied: requests in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/.venv/lib/python3.10/site-packages (from transformers>=4.40.0->trl==0.11.4) (4.67.1)\n",
      "Collecting docstring-parser>=0.15 (from tyro>=0.5.11->trl==0.11.4)\n",
      "  Downloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /root/.venv/lib/python3.10/site-packages (from tyro>=0.5.11->trl==0.11.4) (14.0.0)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.11.4)\n",
      "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting typeguard>=4.0.0 (from tyro>=0.5.11->trl==0.11.4)\n",
      "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/.venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/.venv/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.11.4) (0.1.2)\n",
      "Requirement already satisfied: psutil in /root/.venv/lib/python3.10/site-packages (from accelerate->trl==0.11.4) (7.0.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /root/.venv/lib/python3.10/site-packages (from datasets->trl==0.11.4) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /root/.venv/lib/python3.10/site-packages (from datasets->trl==0.11.4) (0.3.8)\n",
      "Requirement already satisfied: pandas in /root/.venv/lib/python3.10/site-packages (from datasets->trl==0.11.4) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /root/.venv/lib/python3.10/site-packages (from datasets->trl==0.11.4) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /root/.venv/lib/python3.10/site-packages (from datasets->trl==0.11.4) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /root/.venv/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (3.11.18)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /root/.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /root/.venv/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets->trl==0.11.4) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/.venv/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl==0.11.4) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/.venv/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/.venv/lib/python3.10/site-packages (from requests->transformers>=4.40.0->trl==0.11.4) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/.venv/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl==0.11.4) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/.venv/lib/python3.10/site-packages (from pandas->datasets->trl==0.11.4) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/.venv/lib/python3.10/site-packages (from pandas->datasets->trl==0.11.4) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/.venv/lib/python3.10/site-packages (from pandas->datasets->trl==0.11.4) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl==0.11.4) (1.17.0)\n",
      "Downloading trl-0.11.4-py3-none-any.whl (316 kB)\n",
      "Downloading tyro-0.9.20-py3-none-any.whl (125 kB)\n",
      "Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: typeguard, shtab, docstring-parser, tyro, trl\n",
      "\u001b[2K  Attempting uninstall: trl\n",
      "\u001b[2K    Found existing installation: trl 0.17.0\n",
      "\u001b[2K    Uninstalling trl-0.17.0:\n",
      "\u001b[2K      Successfully uninstalled trl-0.17.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [trl]\u001b[32m4/5\u001b[0m [trl]\n",
      "\u001b[1A\u001b[2KSuccessfully installed docstring-parser-0.16 shtab-1.7.2 trl-0.11.4 typeguard-4.4.2 tyro-0.9.20\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch\n",
    "# !pip install numpy\n",
    "# !pip install openai\n",
    "# !pip install tenacity\n",
    "# !pip install tiktoken\n",
    "# !pip install transformers\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install bitsandbytes\n",
    "# !pip install datasets\n",
    "# !pip install sentencepiece\n",
    "# !pip install peft\n",
    "# !pip install evaluate\n",
    "# !pip install trl==0.11.4\n",
    "# !pip install protobuf\n",
    "# !pip install python-dotenv\n",
    "# !pip install pandas_ta\n",
    "# !pip install ollama\n",
    "# !pip install accelerate\n",
    "# !pip install ipywidgets\n",
    "# !pip install pynvml\n",
    "# !pip uninstall torch torchvision torchaudio -y\n",
    "# !pip install torch==2.5.0 torchvision==0.20.0 torchaudio==2.5.0 --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==2.2.6\n",
      "openai==1.79.0\n",
      "tenacity==9.1.2\n",
      "tiktoken==0.9.0\n",
      "transformers==4.51.3\n",
      "pandas==2.2.3\n",
      "scikit-learn==1.6.1\n",
      "torch==2.5.0+cu124\n",
      "bitsandbytes==0.45.5\n",
      "datasets==3.6.0\n",
      "sentencepiece==0.2.0\n",
      "peft==0.15.2\n",
      "evaluate==0.4.3\n",
      "trl==0.17.0\n",
      "protobuf==6.31.0\n",
      "python-dotenv==1.1.0\n",
      "pandas_ta==0.3.14b0\n",
      "ollama==0.4.8\n",
      "accelerate==1.7.0\n",
      "ipywidgets==8.1.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1786/316775683.py:1: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "libs = [\n",
    "    \"numpy\", \"openai\", \"tenacity\", \"tiktoken\", \"transformers\", \"pandas\",\n",
    "    \"scikit-learn\", \"torch\", \"bitsandbytes\", \"datasets\", \"sentencepiece\",\n",
    "    \"peft\", \"evaluate\", \"trl\", \"protobuf\", \"python-dotenv\", \"pandas_ta\",\n",
    "    \"ollama\", \"accelerate\", \"ipywidgets\"\n",
    "]\n",
    "\n",
    "for lib in libs:\n",
    "    try:\n",
    "        version = pkg_resources.get_distribution(lib).version\n",
    "        print(f\"{lib}=={version}\")\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print(f\"{lib} not installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes\n",
    "# !pip install sentencepiece\n",
    "# !pip install transformers\n",
    "# !pip install accelerate\n",
    "# !pip install peft\n",
    "# !pip install evaluate\n",
    "# !pip install ipywidgets\n",
    "\n",
    "\n",
    "# # !pip install transformers==4.38.2\n",
    "# # !pip uninstall torch torchvision torchaudio -y\n",
    "# # !pip install torch==2.2.0+cu121 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# # !pip install -U bitsandbytes\n",
    "# # !pip install peft==0.6.2\n",
    "\n",
    "# !pip uninstall accelerate -y\n",
    "# !pip install accelerate\n",
    "\n",
    "# !pip install --upgrade accelerate\n",
    "# !pip install --upgrade torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "GRID P40-24Q\n"
     ]
    }
   ],
   "source": [
    "import torch # type: ignore\n",
    "print(torch.cuda.is_available())  # Nếu trả về False, CUDA chưa hoạt động\n",
    "print(torch.cuda.device_count())  # Kiểm tra số lượng GPU\n",
    "print(torch.cuda.get_device_name(0))  # Hiển thị tên GPU\n",
    "# print(torch.set_default_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "Namespace(price_dir='data/price/preprocessed/', tweet_dir='data/tweet/raw/', seq_len=5, wandb=False, data_path='./data/merge_sample.json', output_path='./saved_models/lora-DeepSeek-R1-Distill-Qwen', model_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', eval_steps=200, save_steps=200, resume_from_supervised_checkpoint=None, ignore_data_skip='False', num_reflect_trials=2, datasets_dir='./datasets/', local_rank=0, resume_from_reward_checkpoint=False, deepspeed=None, per_device_train_batch_size=4, per_device_eval_batch_size=4, reward_gradient_accumulation_steps=8, reward_learning_rate=3e-05, weight_decay=0.001, reward_base_model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', bf16=False, num_train_epochs=2, train_subset=100000, eval_subset=50000, gradient_checkpointing=True, optim='adamw_torch', lr_scheduler_type='cosine', reward_adapter='./saved_models/reward_model_deepseek-r1-distill-qwen', rl_base_model='./saved_models/lora-DeepSeek-R1-Distill-Qwen-adapter-merged', tokenizer_name='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', reward_model_name='./saved_models/reward_model_deepseek-r1-distill-qwen-adapter-merged', log_with=None, rl_learning_rate=2e-05, output_max_length=128, mini_batch_size=4, batch_size=128, ppo_epochs=4, rl_gradient_accumulation_steps=32, adafactor=False, early_stopping=True, target_kl=0.1, reward_baseline=0, batched_gen=True, save_freq=None, output_dir='./saved_models/tuning_deepseek_r1_distill_qwen_checkpoints/', seed=0, num_shots=4, save_dir='results/')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./data/merge_sample.json'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model\n",
    ")\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import transformers\n",
    "import warnings\n",
    "from datasets import load_dataset\n",
    "from predict_module import sft_dataloader\n",
    "\n",
    "# Thiết lập seed\n",
    "fix_seed = 100\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "# Cấu hình tham số cho huấn luyện\n",
    "args = argparse.Namespace(\n",
    "    price_dir=\"data/price/preprocessed/\",  # Thư mục dữ liệu giá\n",
    "    tweet_dir=\"data/tweet/raw/\",  # Thư mục dữ liệu tweet\n",
    "    seq_len=5,  # Độ dài chuỗi đầu vào\n",
    "    wandb=False,  # Tắt logging với Weights & Biases\n",
    "    data_path=\"./data/merge_sample.json\",  # Đường dẫn file dữ liệu\n",
    "    output_path=\"./saved_models/lora-DeepSeek-R1-Distill-Qwen\",  # Thư mục lưu mô hình LoRA\n",
    "    model_path=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # Mô hình DeepSeek\n",
    "    eval_steps=200,  # Số bước đánh giá\n",
    "    save_steps=200,  # Số bước lưu checkpoint\n",
    "    resume_from_supervised_checkpoint=None,  # Không resume từ checkpoint\n",
    "    ignore_data_skip=\"False\",  # Không bỏ qua dữ liệu khi resume\n",
    "    num_reflect_trials=2,  # Số lần thử phản ánh\n",
    "    datasets_dir=\"./datasets/\",  # Thư mục datasets\n",
    "    local_rank=0,  # Rank cục bộ cho DDP\n",
    "    resume_from_reward_checkpoint=False,  # Không resume từ reward checkpoint\n",
    "    deepspeed=None,  # Không dùng DeepSpeed\n",
    "    per_device_train_batch_size=4,  # Batch size huấn luyện trên mỗi GPU\n",
    "    per_device_eval_batch_size=4,  # Batch size đánh giá trên mỗi GPU\n",
    "    reward_gradient_accumulation_steps=8,  # Số bước tích lũy gradient cho reward\n",
    "    reward_learning_rate=3e-5,  # Learning rate cho reward\n",
    "    weight_decay=0.001,  # Trọng số giảm dần\n",
    "    reward_base_model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # Mô hình reward\n",
    "    bf16=False,  # Sử dụng fp16 thay vì bf16\n",
    "    num_train_epochs=2,  # Số epoch huấn luyện\n",
    "    train_subset=100000,  # Số mẫu huấn luyện\n",
    "    eval_subset=50000,  # Số mẫu đánh giá\n",
    "    gradient_checkpointing=True,  # Bật gradient checkpointing để tiết kiệm VRAM\n",
    "    optim=\"adamw_torch\",  # Optimizer AdamW từ PyTorch\n",
    "    lr_scheduler_type=\"cosine\",  # Lịch trình learning rate kiểu cosine\n",
    "    reward_adapter=\"./saved_models/reward_model_deepseek-r1-distill-qwen\",  # Adapter reward\n",
    "    rl_base_model=\"./saved_models/lora-DeepSeek-R1-Distill-Qwen-adapter-merged\",  # Mô hình RL\n",
    "    tokenizer_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # Tokenizer\n",
    "    reward_model_name=\"./saved_models/reward_model_deepseek-r1-distill-qwen-adapter-merged\",  # Mô hình reward merged\n",
    "    log_with=None,  # Không dùng logging cụ thể\n",
    "    rl_learning_rate=2e-5,  # Learning rate cho RL\n",
    "    output_max_length=128,  # Độ dài đầu ra tối đa\n",
    "    mini_batch_size=4,  # Kích thước mini-batch\n",
    "    batch_size=128,  # Kích thước batch tổng\n",
    "    ppo_epochs=4,  # Số epoch cho PPO\n",
    "    rl_gradient_accumulation_steps=32,  # Số bước tích lũy gradient cho RL\n",
    "    adafactor=False,  # Không dùng Adafactor\n",
    "    early_stopping=True,  # Bật early stopping\n",
    "    target_kl=0.1,  # KL target cho RL\n",
    "    reward_baseline=0,  # Baseline cho reward\n",
    "    batched_gen=True,  # Tạo batch\n",
    "    save_freq=None,  # Tần suất lưu\n",
    "    output_dir=\"./saved_models/tuning_deepseek_r1_distill_qwen_checkpoints/\",  # Thư mục lưu checkpoint\n",
    "    seed=0,  # Seed cho RL\n",
    "    num_shots=4,  # Số shots cho few-shot\n",
    "    save_dir=\"results/\"  # Thư mục lưu kết quả\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Args in experiment:\")\n",
    "print(args)\n",
    "\n",
    "args.data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee62fe52f3064da18c7bccdaac082507",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', 'input_ids', 'labels', 'attention_mask'],\n",
       "    num_rows: 30\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "DATA_PATH = args.data_path\n",
    "\n",
    "data = load_dataset(\"json\", data_files=DATA_PATH)\n",
    "data['train'][1]\n",
    "\n",
    "# --- Tải tokenizer ---\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    args.model_path,\n",
    "    add_eos_token=True,  # Thêm token kết thúc\n",
    "    local_files_only=args.offline if hasattr(args, 'offline') else False,  # Chế độ offline\n",
    "    trust_remote_code=True  # Cho DeepSeek\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Gán pad_token\n",
    "\n",
    "\n",
    "CUTOFF_LEN = 256  # Độ dài chuỗi tối đa\n",
    "data = load_dataset(\"json\", data_files=DATA_PATH)  # Tải JSON dataset\n",
    "val_set_size = int(0.1 * len(data[\"train\"]))  # Tính validation size\n",
    "print(data)  # In thông tin dataset\n",
    "\n",
    "# --- Tải dữ liệu huấn luyện và đánh giá ---\n",
    "dataloader = sft_dataloader.SFTDataLoader(data, CUTOFF_LEN, val_set_size, tokenizer)  # Định dạng và tokenize\n",
    "train_data, val_data = dataloader.load_data()  # Chia train/validation\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải mô hình từ: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472abf9014d24684937b81a2d3ec69a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39358fa04e5148bd94d27188072bf622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78bd10dc32b44aeb498f813a11ac8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27b99f944e214ebf8baaa9747b717f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a96025f16845b9b045df0b8b6aa80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e189eea235e4f09baeed48837dfa6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 33\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79c9abfef5b4b4db0a8d1e6bf30ba96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e512d566644107842a80c08ff72a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cấu hình LoRA: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', revision=None, inference_mode=True, r=16, target_modules={'v_proj', 'k_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "Lưu mô hình gộp tại: ./saved_models/lora-DeepSeek-R1-Distill-Qwen-adapter-merged\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "# from trl import SFTTrainer\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, set_peft_model_state_dict\n",
    "\n",
    "\n",
    "def supervised_finetune(args):\n",
    "    # --- Các hằng số huấn luyện ---\n",
    "    MICRO_BATCH_SIZE = args.per_device_train_batch_size  # Batch size mỗi GPU\n",
    "    BATCH_SIZE = args.batch_size  # Batch size tổng\n",
    "    MAX_STEPS = None  # Số bước tối đa, tính động\n",
    "    GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE  # Bước tích lũy gradient\n",
    "    EPOCHS = args.num_train_epochs  # Số epoch\n",
    "    LEARNING_RATE = 3e-4  # Tốc độ học\n",
    "    CUTOFF_LEN = 256  # Độ dài chuỗi tối đa\n",
    "    LORA_R = 16  # Rank LoRA\n",
    "    LORA_ALPHA = 32  # Hệ số scale LoRA\n",
    "    LORA_DROPOUT = 0.05  # Dropout LoRA\n",
    "    VAL_PCT = 0.1  # Tỷ lệ validation\n",
    "    TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Layer áp dụng LoRA\n",
    "    DATA_PATH = args.data_path  # Đường dẫn dữ liệu\n",
    "    OUTPUT_DIR = args.output_path  # Thư mục lưu mô hình\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))  # Số GPU (DDP)\n",
    "\n",
    "\n",
    "    # --- Xử lý DDP ---\n",
    "    ddp = world_size != 1  # Kiểm tra đa GPU\n",
    "    if ddp:\n",
    "        torch.cuda.set_device(int(os.environ.get(\"LOCAL_RANK\", 0)))  # Gán GPU\n",
    "        GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size  # Chia tích lũy gradient\n",
    "\n",
    "    #==============================================================================================================================\n",
    "    print(f\"Đang tải mô hình từ: {args.model_path}\")  # In đường dẫn mô hình\n",
    "\n",
    "    # Step 3: Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        args.model_path,\n",
    "        add_eos_token=True,  # Thêm token kết thúc\n",
    "        local_files_only=args.offline if hasattr(args, 'offline') else False  # Chế độ offline\n",
    "        )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Gán pad_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_path, \n",
    "        torch_dtype=torch.float16, \n",
    "        device_map=\"auto\")\n",
    "\n",
    "    # --- Tải dữ liệu ---\n",
    "    dataset = load_dataset(\"json\", data_files=DATA_PATH)  # Tải JSON dataset\n",
    "\n",
    "\n",
    "    val_set_size = int(VAL_PCT * len(dataset[\"train\"]))  # Tính validation size\n",
    "    print(dataset)  # In thông tin dataset\n",
    "\n",
    "\n",
    "    # --- Tải dữ liệu huấn luyện và đánh giá ---\n",
    "    dataloader = sft_dataloader.SFTDataLoader(dataset, CUTOFF_LEN, val_set_size, tokenizer)  # Định dạng và tokenize\n",
    "    train_data, val_data = dataloader.load_data()  # Chia train/validation\n",
    "    train_data, val_data\n",
    "\n",
    "    # Step 4: Configure LoRA\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,  # Rank LoRA\n",
    "        lora_alpha=LORA_ALPHA,  # Scale LoRA\n",
    "        target_modules=TARGET_MODULES,  # Layer LoRA\n",
    "        lora_dropout=LORA_DROPOUT,  # Dropout\n",
    "        bias=\"none\",  # Không bias\n",
    "        task_type=\"CAUSAL_LM\"  # Tác vụ ngôn ngữ\n",
    "    )\n",
    "\n",
    "\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "\n",
    "    # --- Tính max_steps ---\n",
    "    now_max_steps = max((len(dataset[\"train\"]) - val_set_size) // BATCH_SIZE * EPOCHS, EPOCHS)  # Số bước tối đa\n",
    "\n",
    "\n",
    "    # --- Xử lý checkpoint ---\n",
    "    if args.resume_from_supervised_checkpoint:  # Nếu có checkpoint\n",
    "        checkpoint_name = os.path.join(args.resume_from_supervised_checkpoint, \"pytorch_model.bin\")  # Đường dẫn checkpoint\n",
    "        if not os.path.exists(checkpoint_name):\n",
    "            pytorch_bin_path = checkpoint_name\n",
    "            checkpoint_name = os.path.join(args.resume_from_supervised_checkpoint, \"adapter_model.bin\")  # Kiểm tra file khác\n",
    "            if os.path.exists(checkpoint_name):\n",
    "                os.rename(checkpoint_name, pytorch_bin_path)  # Đổi tên\n",
    "                warnings.warn(\"Đã đổi tên 'adapter_model.bin' thành 'pytorch_model.bin'\")\n",
    "            else:\n",
    "                args.resume_from_supervised_checkpoint = None  # Bỏ resume\n",
    "        if os.path.exists(checkpoint_name):\n",
    "            print(f\"Tiếp tục từ: {checkpoint_name}\")\n",
    "            adapters_weights = torch.load(checkpoint_name)  # Tải LoRA\n",
    "            model = set_peft_model_state_dict(model, adapters_weights)  # Áp dụng\n",
    "        else:\n",
    "            print(f\"Không tìm thấy: {checkpoint_name}\")\n",
    "        train_args_path = os.path.join(args.resume_from_supervised_checkpoint, \"trainer_state.json\")  # File trạng thái\n",
    "        if os.path.exists(train_args_path):\n",
    "            base_train_args = json.load(open(train_args_path, 'r'))\n",
    "            base_max_steps = base_train_args[\"max_steps\"]  # Số bước cũ\n",
    "            resume_scale = base_max_steps / now_max_steps\n",
    "            if base_max_steps > now_max_steps:\n",
    "                warnings.warn(f\"Thay epoch {EPOCHS} bằng {base_max_steps}\")\n",
    "                EPOCHS = None\n",
    "                MAX_STEPS = base_max_steps\n",
    "            else:\n",
    "                MAX_STEPS = now_max_steps\n",
    "    else:\n",
    "        MAX_STEPS = now_max_steps\n",
    "\n",
    "\n",
    "    # Step 5: Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,  # Directory to save results\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,  # Batch size GPU\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Tích lũy gradient\n",
    "        warmup_steps=100,  # Bước khởi động\n",
    "        num_train_epochs=EPOCHS if EPOCHS else 1,  # Epoch\n",
    "        max_steps=MAX_STEPS,  # Số bước tối đa\n",
    "        learning_rate=LEARNING_RATE,  # Tốc độ học\n",
    "        bf16=args.bf16,  # BF16\n",
    "        fp16=not args.bf16,  # FP16\n",
    "        logging_steps=20,  # Log mỗi 20 bước\n",
    "        eval_strategy=\"steps\" if val_set_size > 0 else \"no\",  # Đánh giá\n",
    "        save_strategy=\"steps\",  # Lưu checkpoint\n",
    "        eval_steps=args.eval_steps if val_set_size > 0 else None,  # Bước đánh giá\n",
    "        save_steps=args.save_steps,  # Bước lưu\n",
    "        save_total_limit=30,  # Số checkpoint tối đa\n",
    "        load_best_model_at_end=True if val_set_size > 0 else False,  # Tải mô hình tốt\n",
    "        ddp_find_unused_parameters=False if ddp else None,  # Tối ưu DDP\n",
    "        report_to=\"wandb\" if args.wandb else [],  # Báo cáo WandB\n",
    "        optim=args.optim,  # Bộ tối ưu\n",
    "        lr_scheduler_type=args.lr_scheduler_type,  # Scheduler\n",
    "        remove_unused_columns=True,  # Xóa cột thừa\n",
    "        max_grad_norm=1.0,  # Giới hạn gradient\n",
    "    )\n",
    "\n",
    "\n",
    "    # Step 6: Initialize the trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_data, \n",
    "        eval_dataset=val_data,  # Small evaluation set\n",
    "        data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    "    )\n",
    "\n",
    "    # Step 7: Train the model\n",
    "    trainer.train(resume_from_checkpoint=args.resume_from_supervised_checkpoint)\n",
    "    \n",
    "    model.save_pretrained(OUTPUT_DIR) \n",
    "\n",
    "# --- Chạy huấn luyện ---\n",
    "\n",
    "supervised_finetune(args)\n",
    "\n",
    "from predict_module.merge_peft_adapter import merge_peft_adapter\n",
    "\n",
    "# --- Gộp adapter LoRA ---\n",
    "merge_peft_adapter(model_name=args.output_path, output_name=args.rl_base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Giải phóng bộ nhớ trên GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name: ./datasets/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 546,304 || all params: 1,544,262,144 || trainable%: 0.0354\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e140d4523e6f4fd9b003c1b0ac157272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6ba78895aaf42eab3464e662ca47c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataset:  6\n",
      "eval_dataset:  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da427bf368ea4f3c8e1feefdb708b484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_dataset:  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e3522b80ee4245b73c35e011689ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving last checkpoint of the model\n",
      "Cấu hình LoRA: LoraConfig(task_type='SEQ_CLS', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', revision=None, inference_mode=True, r=4, target_modules={'v_proj', 'q_proj'}, exclude_modules=None, lora_alpha=8, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=['classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)\n",
      "Lưu mô hình gộp tại: ./saved_models/reward_model_deepseek-r1-distill-qwen-adapter-merged\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from transformers.utils import PaddingStrategy\n",
    "\n",
    "from predict_module import rm_dataloader\n",
    "\n",
    "def train_reward_model(args):\n",
    "    script_args = args\n",
    "    dataset_name = script_args.datasets_dir\n",
    "    print(\"dataset_name:\", dataset_name)\n",
    "    \n",
    "    output_name = script_args.reward_adapter\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_name,\n",
    "        learning_rate=script_args.reward_learning_rate,\n",
    "        per_device_train_batch_size=1,  # Giảm batch size\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=script_args.num_train_epochs,\n",
    "        weight_decay=script_args.weight_decay,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=200,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=200,\n",
    "        save_total_limit=2,\n",
    "        gradient_accumulation_steps=16,  # Tăng gradient accumulation\n",
    "        gradient_checkpointing=False,  # Tắt gradient checkpointing\n",
    "        deepspeed=None,  # Tắt DeepSpeed để kiểm tra\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[],\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=10,\n",
    "        optim=script_args.optim,\n",
    "        lr_scheduler_type=script_args.lr_scheduler_type,\n",
    "        report_to=\"none\",\n",
    "        no_cuda=False,  # Đảm bảo dùng GPU\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.reward_base_model, trust_remote_code=True)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        script_args.reward_base_model,\n",
    "        num_labels=1,\n",
    "        torch_dtype=torch.bfloat16,  # Thử bfloat16\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # # Handle pad token\n",
    "    # if tokenizer.pad_token is None:\n",
    "    #     tokenizer.pad_token = tokenizer.eos_token\n",
    "    #     model.config.pad_token_id = tokenizer.eos_token_id\n",
    "    # else:\n",
    "    #     model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Set device\n",
    "    # device_map = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # print(\"device_map:\", device_map)\n",
    "    # model = model.to(device_map)  # Chuyển thủ công\n",
    "    \n",
    "    # LoRA config\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        inference_mode=False,\n",
    "        r=4,  # Giảm r\n",
    "        lora_alpha=8,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    num_proc = 1\n",
    "    reward_dataloder = rm_dataloader.RewardDataLoader(dataset_name, script_args.train_subset, script_args.eval_subset, num_proc, tokenizer)\n",
    "    train_dataset, eval_dataset = reward_dataloder.load_data()\n",
    "    \n",
    "    @dataclass\n",
    "    class RewardDataCollatorWithPadding:\n",
    "        tokenizer: PreTrainedTokenizerBase\n",
    "        padding: Union[bool, str, PaddingStrategy] = True\n",
    "        max_length: Optional[int] = None\n",
    "        pad_to_multiple_of: Optional[int] = None\n",
    "        return_tensors: str = \"pt\"\n",
    "\n",
    "        def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "            features_j = []\n",
    "            features_k = []\n",
    "            for feature in features:\n",
    "                features_j.append(\n",
    "                    {\n",
    "                        \"input_ids\": feature[\"input_ids_j\"],\n",
    "                        \"attention_mask\": feature[\"attention_mask_j\"],\n",
    "                    }\n",
    "                )\n",
    "                features_k.append(\n",
    "                    {\n",
    "                        \"input_ids\": feature[\"input_ids_k\"],\n",
    "                        \"attention_mask\": feature[\"attention_mask_k\"],\n",
    "                    }\n",
    "                )\n",
    "            batch_j = self.tokenizer.pad(\n",
    "                features_j,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=self.return_tensors,\n",
    "            )\n",
    "            batch_k = self.tokenizer.pad(\n",
    "                features_k,\n",
    "                padding=self.padding,\n",
    "                max_length=self.max_length,\n",
    "                pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                return_tensors=self.return_tensors,\n",
    "            )\n",
    "            batch = {\n",
    "                \"input_ids_j\": batch_j[\"input_ids\"],\n",
    "                \"attention_mask_j\": batch_j[\"attention_mask\"].to(dtype=torch.bfloat16),\n",
    "                \"input_ids_k\": batch_k[\"input_ids\"],\n",
    "                \"attention_mask_k\": batch_k[\"attention_mask\"].to(dtype=torch.bfloat16),\n",
    "                \"return_loss\": True,\n",
    "            }\n",
    "            return batch\n",
    "    \n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, _ = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=0)\n",
    "        labels = np.zeros(predictions.shape)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    class RewardTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "            rewards_j = model(\n",
    "                input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n",
    "            rewards_k = model(\n",
    "                input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n",
    "            loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "            if return_outputs:\n",
    "                return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "            return loss\n",
    "    \n",
    "    trainer = RewardTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=RewardDataCollatorWithPadding(\n",
    "            tokenizer=tokenizer, max_length=512, pad_to_multiple_of=8),\n",
    "    )\n",
    "    \n",
    "    model.config.use_cache = True\n",
    "    trainer.train(script_args.resume_from_reward_checkpoint)\n",
    "    \n",
    "    print(\"Saving last checkpoint of the model\")\n",
    "    model.save_pretrained(output_name)\n",
    "\n",
    "train_reward_model(args)\n",
    "\n",
    "from predict_module.merge_peft_adapter import merge_peft_adapter\n",
    "merge_peft_adapter(model_name=args.reward_adapter, output_name=args.reward_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import DataCollatorWithPadding, AutoTokenizer\n",
    "\n",
    "\n",
    "# def build_dataset(tokenizer, dataset_name, input_min_text_length=2, input_max_text_length=8):\n",
    "#     \"\"\"\n",
    "#     Tạo dataset cho huấn luyện PPO.\n",
    "    \n",
    "#     Args:\n",
    "#         tokenizer: Tokenizer để mã hóa văn bản.\n",
    "#         dataset_name: Tên hoặc đường dẫn dataset.\n",
    "#         input_min_text_length: Độ dài tối thiểu của câu hỏi.\n",
    "#         input_max_text_length: Độ dài tối đa của câu hỏi.\n",
    "    \n",
    "#     Returns:\n",
    "#         Dataset đã được xử lý với các cột query và input_ids.\n",
    "#     \"\"\"\n",
    "#     ds = load_dataset(dataset_name, split=\"train\")\n",
    "#     original_columns = ds.column_names\n",
    "#     def preprocess_function(examples):\n",
    "#         new_examples = {\n",
    "#             \"query\": [],\n",
    "#             \"input_ids\": [],\n",
    "#         }\n",
    "#         # Giả định dataset có cột 'user_input' hoặc 'question'\n",
    "#         input_key = \"user_input\" if \"user_input\" in examples else \"question\"\n",
    "#         for question in examples[input_key]:\n",
    "#             query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "#             tokenized_question = tokenizer(\n",
    "#                 query,\n",
    "#                 truncation=True,\n",
    "#                 max_length=512,\n",
    "#                 return_tensors=\"pt\"\n",
    "#             )\n",
    "#             new_examples[\"query\"].append(query)\n",
    "#             new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"].squeeze(0))\n",
    "#         return new_examples\n",
    "\n",
    "#     ds = ds.map(\n",
    "#         preprocess_function,\n",
    "#         batched=True,\n",
    "#         num_proc=1,\n",
    "#         remove_columns=original_columns,\n",
    "#     )\n",
    "#     ds.set_format(type=\"torch\")\n",
    "#     return ds\n",
    "\n",
    "#     # Tạo dataset\n",
    "#     dataset = build_dataset(tokenizer, dataset_name=dataset_name)\n",
    "\n",
    "# dataset_name = args.datasets_dir\n",
    "\n",
    "# ds = load_dataset(dataset_name, split=\"train\")\n",
    "# ds\n",
    "\n",
    "# tokenizer_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# # Tải tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, trust_remote_code=True)\n",
    "    \n",
    "# dataset = build_dataset(tokenizer, dataset_name=dataset_name)\n",
    "\n",
    "# dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_model_name: ./saved_models/reward_model_deepseek-r1-distill-qwen-adapter-merged\n",
      "dataset_name: ./datasets/\n",
      "model_name: ./saved_models/lora-DeepSeek-R1-Distill-Qwen-adapter-merged\n",
      "train_dataset size: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.10/site-packages/trl/trainer/ppo_config.py:207: FutureWarning: `PPOConfig` is deprecated and will be removed in the future. Please use `PPOv2Config` with `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: LlamaTokenizerFast\n",
      "Dataset created with 64 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetune model: ./saved_models/lora-DeepSeek-R1-Distill-Qwen-adapter-merged <class 'trl.models.modeling_value_head.AutoModelForCausalLMWithValueHead'>\n",
      "Dataset({\n",
      "    features: ['query', 'input_ids'],\n",
      "    num_rows: 64\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:193: FutureWarning: `PPOTrainer` is deprecated and will be removed in trl v0.12. Please use `PPOv2Trainer` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at ./saved_models/reward_model_deepseek-r1-distill-qwen-adapter-merged and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda:0\n",
      "0it [00:00, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/root/.venv/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "10it [05:19, 32.31s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "16it [08:49, 33.10s/it]\n",
      "/root/.venv/lib/python3.10/site-packages/trl/trainer/ppo_trainer.py:1431: UserWarning: Cannot retrieve user information assuming you are running in offline mode.\n",
      "  warnings.warn(\"Cannot retrieve user information assuming you are running in offline mode.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final checkpoint saved at: ./saved_models/tuning_deepseek_r1_distill_qwen_checkpoints/step_saved\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig\n",
    "from tqdm import tqdm\n",
    "from transformers import Adafactor, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, AutoModelForCausalLM, DataCollatorWithPadding\n",
    "from transformers import GenerationConfig, pipeline\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from trl.core import LengthSampler\n",
    "from trl import create_reference_model\n",
    "import os\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "def tuning_lm_with_rl(args):\n",
    "    # Khởi tạo script_args từ args\n",
    "    script_args = args\n",
    "    reward_model_name = script_args.reward_model_name\n",
    "    print(\"reward_model_name:\", reward_model_name)\n",
    "\n",
    "    # Đường dẫn dataset\n",
    "    dataset_name = script_args.datasets_dir\n",
    "    print(\"dataset_name:\", dataset_name)\n",
    "\n",
    "    # Cấu hình PPO\n",
    "    config = PPOConfig(\n",
    "        learning_rate=script_args.rl_learning_rate,\n",
    "        # batch_size=script_args.batch_size,\n",
    "        batch_size=4,\n",
    "        # mini_batch_size=script_args.mini_batch_size,\n",
    "        mini_batch_size=2,\n",
    "        # gradient_accumulation_steps=script_args.rl_gradient_accumulation_steps,\n",
    "        gradient_accumulation_steps=1,\n",
    "        ppo_epochs=script_args.ppo_epochs,  \n",
    "        seed=script_args.seed,\n",
    "    )\n",
    "\n",
    "    # Tên mô hình gốc\n",
    "    model_name = script_args.rl_base_model\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # Tải dataset\n",
    "    train_dataset = load_dataset(dataset_name, split=\"train\")\n",
    "    print(\"train_dataset size:\", len(train_dataset))\n",
    "\n",
    "    # Cấu hình tham số cho sentiment pipeline\n",
    "    sent_kwargs = {\n",
    "        \"return_all_scores\": True,\n",
    "        \"function_to_apply\": \"none\",\n",
    "        \"batch_size\": 1,\n",
    "        \"truncation\": True\n",
    "    }\n",
    "\n",
    "    # Tải tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(script_args.tokenizer_name, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    print(\"Tokenizer loaded:\", tokenizer.__class__.__name__)\n",
    "\n",
    "    def build_dataset(tokenizer, dataset_name, input_min_text_length=2, input_max_text_length=8):\n",
    "        \"\"\"\n",
    "        Tạo dataset cho huấn luyện PPO.\n",
    "        \n",
    "        Args:\n",
    "            tokenizer: Tokenizer để mã hóa văn bản.\n",
    "            dataset_name: Tên hoặc đường dẫn dataset.\n",
    "            input_min_text_length: Độ dài tối thiểu của câu hỏi.\n",
    "            input_max_text_length: Độ dài tối đa của câu hỏi.\n",
    "        \n",
    "        Returns:\n",
    "            Dataset đã được xử lý với các cột query và input_ids.\n",
    "        \"\"\"\n",
    "        ds = load_dataset(dataset_name, split=\"train\")\n",
    "        # Giả sử dataset của bạn tên là `ds`, hiện có 6 dòng\n",
    "        repeat_factor = 64 // len(ds) + 1  # Lặp đủ số lần để vượt 64\n",
    "\n",
    "        # Lặp lại nhiều lần rồi cắt còn đúng 64\n",
    "        ds = concatenate_datasets([ds] * repeat_factor)\n",
    "        ds = ds.select(range(64))  # Lấy đúng 64 dòng\n",
    "        original_columns = ds.column_names\n",
    "\n",
    "        def preprocess_function(examples):\n",
    "            new_examples = {\n",
    "                \"query\": [],\n",
    "                \"input_ids\": [],\n",
    "            }\n",
    "            # Giả định dataset có cột 'user_input' hoặc 'question'\n",
    "            input_key = \"user_input\" if \"user_input\" in examples else \"question\"\n",
    "            for question in examples[input_key]:\n",
    "                query = \"Question: \" + question + \"\\n\\nAnswer: \"\n",
    "                tokenized_question = tokenizer(\n",
    "                    query,\n",
    "                    truncation=True,\n",
    "                    max_length=512,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                new_examples[\"query\"].append(query)\n",
    "                new_examples[\"input_ids\"].append(tokenized_question[\"input_ids\"].squeeze(0))\n",
    "            return new_examples\n",
    "\n",
    "        ds = ds.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=1,\n",
    "            remove_columns=original_columns,\n",
    "        )\n",
    "        ds.set_format(type=\"torch\")\n",
    "        return ds\n",
    "\n",
    "    # Tạo dataset\n",
    "    dataset = build_dataset(tokenizer, dataset_name=dataset_name)\n",
    "    print(\"Dataset created with\", len(dataset), \"samples\")\n",
    "\n",
    "    def collator(data):\n",
    "        \"\"\"\n",
    "        Collator để xử lý batch dữ liệu.\n",
    "        \"\"\"\n",
    "        return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "    # def collator(data):\n",
    "    #     \"\"\"\n",
    "    #     Custom collator cho PPOTrainer.\n",
    "    #     Dữ liệu đầu vào là list[dict], mỗi dict chứa:\n",
    "    #         - \"query\": str\n",
    "    #         - \"input_ids\": Tensor (1D)\n",
    "    #     \"\"\"\n",
    "    #     input_ids = [item[\"input_ids\"] for item in data]\n",
    "    #     queries = [item[\"query\"] for item in data]\n",
    "\n",
    "    #     # Pad input_ids thủ công nếu cần\n",
    "    #     input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    #     return {\n",
    "    #         \"input_ids\": input_ids,\n",
    "    #         \"query\": queries,\n",
    "    #     }\n",
    "\n",
    "    # Đặt seed để đảm bảo tính tái lập\n",
    "    torch.manual_seed(config.seed)\n",
    "\n",
    "    # Cấu hình LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    "    )\n",
    "\n",
    "    # Tải mô hình chính với value head\n",
    "    model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,  # Đồng bộ với reward model\n",
    "        peft_config=lora_config,\n",
    "    )\n",
    "\n",
    "    # Gán thủ công base_model_prefix để tránh lỗi\n",
    "    model.base_model_prefix = \"model\"  # hoặc \"transformer\", tùy thuộc vào tên trong mô hình gốc\n",
    "    # Gán generation_config để tránh lỗi AttributeError\n",
    "    model.generation_config = GenerationConfig.from_pretrained(\n",
    "        \"./saved_models/lora-DeepSeek-R1-Distill-Qwen-adapter-merged\"\n",
    "    )\n",
    "    # model.config.return_dict = True\n",
    "\n",
    "    # try:\n",
    "    #     model.base_model.model.config.return_dict = True\n",
    "    # except:\n",
    "    #     pass\n",
    "\n",
    "    # model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "    # # Tải mô hình giá trị (value model) với value head\n",
    "    # value_model = AutoModelForCausalLM.from_pretrained(\n",
    "    #     model_name,\n",
    "    #     torch_dtype=torch.bfloat16,\n",
    "    #     trust_remote_code=True,\n",
    "    # )\n",
    "    # # value_model = get_peft_model(value_model, lora_config)\n",
    "    # def force_return_dict_recursively(model):\n",
    "    #     \"\"\"\n",
    "    #     Recursively patch forward function of all relevant submodules to enforce return_dict=True.\n",
    "    #     \"\"\"\n",
    "    #     if hasattr(model, 'forward'):\n",
    "    #         original_forward = model.forward\n",
    "\n",
    "    #         def patched_forward(*args, **kwargs):\n",
    "    #             kwargs[\"return_dict\"] = True\n",
    "    #             return original_forward(*args, **kwargs)\n",
    "\n",
    "    #         model.forward = patched_forward\n",
    "\n",
    "    #     # Patch model.base_model.model nếu có\n",
    "    #     if hasattr(model, \"base_model\") and hasattr(model.base_model, \"model\"):\n",
    "    #         force_return_dict_recursively(model.base_model.model)\n",
    "\n",
    "    #     return model\n",
    "\n",
    "    # # Sau khi load:\n",
    "    # model = force_return_dict_recursively(model)\n",
    "    # value_model = force_return_dict_recursively(value_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Tạo generation_config nếu không có\n",
    "    if not hasattr(model, \"generation_config\"):\n",
    "        model.generation_config = GenerationConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    print(\"Finetune model:\", model_name, type(model))\n",
    "\n",
    "    # # Tải reward model từ checkpoint\n",
    "    # reward_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    #     reward_model_name,\n",
    "    #     torch_dtype=torch.bfloat16,\n",
    "    #     trust_remote_code=True,\n",
    "    # )\n",
    "    # reward_model_config = AutoConfig.from_pretrained(reward_model_name, trust_remote_code=True)\n",
    "    # print(\"Reward model:\", type(reward_model))\n",
    "\n",
    "    # Tạo optimizer (nếu dùng Adafactor)\n",
    "    optimizer = None\n",
    "    if script_args.adafactor:\n",
    "        optimizer = Adafactor(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()),\n",
    "            scale_parameter=False,\n",
    "            relative_step=False,\n",
    "            warmup_init=False,\n",
    "            lr=config.learning_rate,\n",
    "        )\n",
    "\n",
    "    # Khởi tạo PPOTrainer\n",
    "    print(dataset)\n",
    "    ppo_trainer = PPOTrainer(\n",
    "        config=config,  # Sử dụng args thay vì config\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,  \n",
    "        # reward_model=reward_model,\n",
    "        # ref_model = None,\n",
    "        # value_model=value_model,\n",
    "        dataset=dataset,\n",
    "        data_collator=collator,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "    \n",
    "    # ppo_trainer.train()\n",
    "\n",
    "    # Xác định thiết bị\n",
    "    device = ppo_trainer.accelerator.device\n",
    "    if ppo_trainer.accelerator.num_processes == 1:\n",
    "        device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Tạo sentiment pipeline\n",
    "    sentiment_pipe = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=reward_model_name,\n",
    "        device_map=\"auto\",\n",
    "        # config=reward_model_config,\n",
    "        tokenizer=tokenizer,\n",
    "        # device=device,\n",
    "    )\n",
    "\n",
    "    # Cấu hình tham số sinh văn bản\n",
    "    generation_kwargs = {\n",
    "        \"top_k\": 0.0,\n",
    "        \"top_p\": 1.0,\n",
    "        \"do_sample\": True,\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    }\n",
    "    output_min_length = 32\n",
    "    output_max_length = script_args.output_max_length\n",
    "    output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
    "\n",
    "    # Vòng lặp huấn luyện PPO\n",
    "    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        question_tensors = batch[\"input_ids\"]\n",
    "\n",
    "        # Sinh phản hồi\n",
    "        response_tensors = ppo_trainer.generate(\n",
    "            question_tensors,\n",
    "            return_prompt=False,\n",
    "            length_sampler=output_length_sampler,\n",
    "            **generation_kwargs,\n",
    "        )\n",
    "        batch[\"response\"] = tokenizer.batch_decode(response_tensors, skip_special_tokens=True)\n",
    "\n",
    "        # Tính điểm thưởng từ reward model\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "        rewards = [torch.tensor(output[0][\"score\"] - script_args.reward_baseline) for output in pipe_outputs]\n",
    "\n",
    "        # Thực hiện bước PPO\n",
    "        stats = ppo_trainer.step(question_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "        # Lưu checkpoint định kỳ\n",
    "        if script_args.save_freq and epoch and epoch % script_args.save_freq == 0:\n",
    "            save_dir = os.path.join(script_args.output_dir, f\"step_{epoch}\")\n",
    "            ppo_trainer.save_pretrained(save_dir)\n",
    "            print(f\"Saved checkpoint at: {save_dir}\")\n",
    "\n",
    "    # Lưu checkpoint cuối cùng\n",
    "    final_save_dir = os.path.join(script_args.output_dir, \"step_saved\")\n",
    "    ppo_trainer.save_pretrained(final_save_dir)\n",
    "    print(f\"Final checkpoint saved at: {final_save_dir}\")\n",
    "\n",
    "# Chạy hàm\n",
    "tuning_lm_with_rl(args)\n",
    "\n",
    "# # Gộp adapter LoRA\n",
    "# from predict_module.merge_peft_adapter import merge_peft_adapter\n",
    "# merge_peft_adapter(\n",
    "#     model_name=os.path.join(args.output_dir, \"step_saved\"),\n",
    "#     output_name=\"./saved_models/sep_model\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
